1. 가상환경생성
cd C:\myLab\Project1\Project1\python
python -m venv myenv
C:\myLab\Project1\Project1\python\myenv\Scripts\activate

2.torch설치(Gpu)
(cpu) pip install torch torchvision torchaudio
print(torch.__version__)
2.4.0+cpu

(pc)
nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Wed_Feb__8_05:53:42_Coordinated_Universal_Time_2023
Cuda compilation tools, release 12.1, V12.1.66
Build cuda_12.1.r12.1/compiler.32415258_0

(gpu)
pip install torch==2.0.0+cu121 torchvision==0.15.1+cu121 torchaudio==2.0.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html
->12.1과 호환되지않아 11.8로 설치
pip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html

(버전확인)
(base) (myenv) C:\myLab\Project1\Project1\python>python
Python 3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> print("PyTorch version:", torch.__version__)
PyTorch version: 2.0.0+cu118
>>> print("CUDA available:", torch.cuda.is_available())
CUDA available: True
>>> print("CUDA version:", torch.version.cuda)
CUDA version: 11.8

3. opencv-python 설치
pip install opencv-python

4. pandas 설치
pip install pandas

5. requests 설치
pip install requests

6. onnx설치
pip install onnx
pip install --upgrade onnx protobuf numpy onnxruntime
pip install onnxruntime
python.exe -m pip install --upgrade pip
conda install -c conda-forge onnx
python myTorchScript.py
python
import onnx
print(onnx.__version__)
1.16.2

7. onnx파일 생성 및 검증
- 생성 : "C:\myLab\Project1\Project1\python\yolov5m.onnx"
(base) (myenv) C:\myLab\Project1\Project1\python>python myTorchScript.py
Model has been exported to ONNX format at C:/myLab/Project1/Project1/python/yolov5m.onnx

# myTorchScript.py
import sys
import torch

# YOLOv5 경로를 추가합니다
sys.path.append('C:/yolov5')

# 모델 로드 (CUDA로)
model_path = 'C:/yolov5/yolov5m.pt'

# CUDA가 사용 가능한 경우, CUDA로 로드하고, 그렇지 않으면 CPU로 로드합니다
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = torch.load(model_path, map_location=device)['model'].float()
model.to(device)  # 모델을 CUDA로 이동
model.eval()

# 더미 입력 텐서 생성 (CUDA 또는 CPU에 맞게 설정)
dummy_input = torch.randn(1, 3, 640, 640).to(device)  # YOLOv5의 기본 입력 크기

# ONNX로 모델 내보내기
onnx_path = 'C:/myLab/Project1/Project1/python/yolov5m.onnx'
torch.onnx.export(
    model,
    dummy_input,
    onnx_path,
    verbose=True,
    opset_version=12,  # ONNX Opset version 변경
    input_names=['input'],
    output_names=['output']
)

print("Model has been exported to ONNX format at", onnx_path)

- 검증
# myTorchScriptCheck.py
import torch
import numpy as np

# PyTorch 모델 로드 (CUDA에서)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model_path = 'C:/yolov5/yolov5m.pt'
model = torch.load(model_path, map_location=device)['model'].float().to(device)
model.eval()

# 더미 입력 텐서 생성 (CUDA 또는 CPU에 맞게 설정)
dummy_input = torch.randn(1, 3, 640, 640).to(device)

# PyTorch 모델로 추론
with torch.no_grad():
    torch_outputs = model(dummy_input)

print("PyTorch model inference completed.")
print("PyTorch model output shape:", torch_outputs.shape)

# PyTorch 결과를 NumPy 배열로 변환
torch_outputs_np = torch_outputs.cpu().numpy()

# 출력 결과 비교
print("Comparing PyTorch and ONNX model outputs...")

# ONNX와 PyTorch 출력 차이 계산
for i, (torch_out, onnx_out) in enumerate(zip(torch_outputs_np, onnx_outputs)):
    diff = np.abs(torch_out - onnx_out).max()
    print(f"Output {i} max difference: {diff}")

    if diff > 1e-5:  # 허용 오차를 설정
        print(f"Warning: Output {i} difference exceeds threshold.")
    else:
        print(f"Output {i} comparison passed.")

(출력) ONNX 모델의 추론이 성공적으로 완료
(base) (myenv) C:\myLab\Project1\Project1\python>python myTorchScriptCheck.py
ONNX model inference completed.
ONNX model output shape: [(1, 25200, 85), (1, 3, 80, 80, 85), (1, 3, 40, 40, 85), (1, 3, 20, 20, 85)]

8. ONNX Runtime C++ 라이브러리 설치
(base) (myenv) C:\myLab\Project1\Project1\python>git clone --recursive https://github.com/microsoft/onnxruntime.git

9. myenv, onnxruntime c:/로 위치 이동
- onnxruntime설치
git clone --recursive https://github.com/microsoft/onnxruntime.git
or
git clone --recursive https://github.com/microsoft/onnxruntime.git C:\onnxruntime
or
onnxruntime-main.zip(https://github.com/microsoft/onnxruntime)
- Cmake
cd C:\onnxruntime\build Release
cmake ..\cmake -DCMAKE_BUILD_TYPE=Release -DONNX_RUNTIME_BUILD_SHARED_LIBS=ON
cmake --build . --config Release
-> C:\onnxruntime\build\Release 생성

cmake ..\cmake -DCMAKE_BUILD_TYPE=Debug -DONNX_RUNTIME_BUILD_SHARED_LIBS=ON
cmake --build . --config Debug
-> C:\onnxruntime\build\Debug 생성

10. GSL설치
- GSL(GNU Scientific Library)
PS C:\> vcpkg install gsl
PS C:\> $env:VCPKG_ROOT="C:\Users\nayou\pytorch\third_party\opentelemetry-cpp\tools\vcpkg"
PS C:\myLab\Project1\Project1\build> cmake  -G "Visual Studio 16 2019" -DCMAKE_TOOLCHAIN_FILE=C:/Users/nayou/vcpkg/scripts/buildsystems/vcpkg.cmake ..
# GStreamer 설정
set(GSTREAMER_ROOT "C:/gstreamer/1.0/msvc_x86_64")
include_directories(
    "${GSTREAMER_ROOT}/include/gstreamer-1.0"
    "${GSTREAMER_ROOT}/include/glib-2.0"
    "${GSTREAMER_ROOT}/lib/glib-2.0/include"
)
link_directories("${GSTREAMER_ROOT}/lib")

# 라이브러리 링크
target_link_libraries(Project1
    Qt6::Widgets
    Qt6::Core
    Qt6::Gui
    ${OpenCV_LIBS}
    image_processing_lib
    imageProcessingLib
    ${CUDA_LIBRARIES}
    ${CUDNN_LIBRARIES}
    ${GSTREAMER_LIBRARIES}
    ${TORCH_ROOT}/lib # LibTorch 링크
    ${ONNX_RUNTIME_LIBRARIES}  # ONNX Runtime 링크
    GSL::gsl 
    GSL::gslcblas
)

- Microsoft의 Guidelines Support Library (GSL)
PS C:\> vcpkg install ms-gsl
설치위치 : C:\Users\nayou\vcpkg\installed\x64-windows
PS C:\> $env:VCPKG_ROOT = "C:\Users\nayou\vcpkg"
PS C:\> vcpkg list
gsl:x64-windows     2.7.1#3      The GNU Scientific Library is a numerical librar..

# Set the path to vcpkg toolchain file
set(CMAKE_TOOLCHAIN_FILE "C:/Users/nayou/vcpkg/scripts/buildsystems/vcpkg.cmake" CACHE FILEPATH "Path to vcpkg toolchain file")

# Add the path to the installed vcpkg packages
set(CMAKE_PREFIX_PATH "C:/Users/nayou/vcpkg/installed/x64-windows" ${CMAKE_PREFIX_PATH})

# Find and link Microsoft.GSL
find_package(Microsoft.GSL REQUIRED)
add_executable(MyProject main.cpp)
target_link_libraries(MyProject PRIVATE Microsoft.GSL::GSL)

PS C:\myLab\Project1\Project1\build> cmake  -G "Visual Studio 16 2019" -DCMAKE_TOOLCHAIN_FILE=C:/Users/nayou/vcpkg/scripts/buildsystems/vcpkg.cmake ..
PS C:\> dir "C:\Users\nayou\vcpkg\installed\x64-windows\share\ms-gsl"


    디렉터리: C:\Users\nayou\vcpkg\installed\x64-windows\share\ms-gsl


Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
-a----      2022-01-29   오전 8:22           1157 copyright
-a----      2024-08-07   오후 9:03           3609 vcpkg.spdx.json
-a----      2024-08-07   오후 9:03            825 vcpkg_abi_info.txt

- 다시  GNU Scientific Library 사용

# Set the path to vcpkg toolchain file
set(CMAKE_TOOLCHAIN_FILE "C:/Users/nayou/vcpkg/scripts/buildsystems/vcpkg.cmake" CACHE FILEPATH "Path to vcpkg toolchain file")

# Add the path to the installed vcpkg packages
set(CMAKE_PREFIX_PATH "C:/Users/nayou/vcpkg/installed/x64-windows" ${CMAKE_PREFIX_PATH})

find_package(GSL REQUIRED)

# 라이브러리 링크
target_link_libraries(Project1
    Qt6::Widgets
    Qt6::Core
    Qt6::Gui
    ${OpenCV_LIBS}
    image_processing_lib
    imageProcessingLib
    ${CUDA_LIBRARIES}
    ${CUDNN_LIBRARIES}
    ${GSTREAMER_LIBRARIES}
    ${TORCH_ROOT}/lib # LibTorch 링크
    ${ONNX_RUNTIME_LIBRARIES}  # ONNX Runtime 링크
    GSL::gsl GSL::gslcblas  # GSL 링크 추가
)

PS C:\> $env:CMAKE_PREFIX_PATH="C:\Users\nayou\vcpkg\installed\x64-windows"
PS C:\myLab\Project1\Project1\build> cmake  -G "Visual Studio 16 2019" -DCMAKE_TOOLCHAIN_FILE=C:/Users/nayou/vcpkg/scripts/buildsystems/vcpkg.cmake ..

