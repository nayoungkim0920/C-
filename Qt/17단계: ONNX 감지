<수정사항>
1. yolov5 best.pt (학습시킨 컵인식 90%이상)
-> https://github.com/nayoungkim0920/SMART_AI_FARM/blob/main/yolov5inPC_CUDA_cuDNN 참고
C:\yolov5\runs\train\exp35\weights\best.pt 파일을
C:\myLab\Project1\Project1\python폴더로 복사
"C:\myLab\Project1\Project1\python\myTorchScript.py" => best.onnx 생성

2. CMakeLists.txt 수정

3. 감지프로그램 구현 및 테스트(OpenCV만)

<코드>
1. C:\myLab\Project1\Project1\python\myTorchScript.py
import sys
import torch
import onnx
import onnxruntime as ort
import os

# YOLOv5 경로를 추가합니다
sys.path.append('C:/yolov5')
print("YOLOv5 path added to system path.")

# 모델 경로 설정
model_path = 'C:/myLab/Project1/Project1/python/best.pt'
print(f"Model path set to: {model_path}")

# CUDA가 사용 가능한 경우, CUDA로 로드하고, 그렇지 않으면 CPU로 로드합니다
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# 모델 로드
try:
    print("Attempting to load the model...")
    model = torch.load(model_path, map_location=device, weights_only=False)['model'].float()
    model.to(device)
    model.eval()
    print("Model loaded and set to evaluation mode.")
except Exception as e:
    print("Error loading model:", str(e))
    sys.exit()  # 모델 로드 실패 시 종료

# 더미 입력 텐서 생성 (CUDA 또는 CPU에 맞게 설정)
dummy_input = torch.randn(1, 3, 640, 640).to(device)
print(f"Dummy input tensor created with shape: {dummy_input.shape}")

# ONNX로 모델 내보내기
onnx_path = 'C:/myLab/Project1/Project1/python/best.onnx'
print(f"Export path set to: {onnx_path}")

try:
    print("Attempting to export model to ONNX format...")
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        verbose=True,
        opset_version=12,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    # 디바이스 정보 출력
    print(f"ONNX export completed. Using device: {device}")

    # 파일 생성 확인
    if os.path.exists(onnx_path):
        print(f"ONNX file successfully created at: {onnx_path}")
    else:
        print(f"Failed to create ONNX file at: {onnx_path}")
except Exception as e:
    print("Error exporting model to ONNX:", str(e))
    sys.exit()  # ONNX 변환 실패 시 종료

# ONNX Runtime 버전과 CUDA 버전 출력
try:
    print("ONNX Runtime version:", ort.__version__)
except Exception as e:
    print("Error getting ONNX Runtime version:", str(e))

try:
    print("CUDA version:", torch.version.cuda)
except Exception as e:
    print("Error getting CUDA version:", str(e))

# ONNX 모델 검증
try:
    print("Attempting to validate the ONNX model...")
    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)
    print("ONNX model is valid.")
except Exception as e:
    print("Error validating ONNX model:", str(e))
    sys.exit()  # 모델 검증 실패 시 종료

# ONNX Runtime 세션 생성 (GPU 사용)
try:
    print("Creating ONNX Runtime session...")
    providers = ['CUDAExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']
    ort_session = ort.InferenceSession(onnx_path, providers=providers)
    print(f"ONNX Runtime session created with {'GPU' if torch.cuda.is_available() else 'CPU'} support.")
except Exception as e:
    print("Error creating ONNX Runtime session:", str(e))
    sys.exit()  # 세션 생성 실패 시 종료

# 더미 입력 준비
dummy_input_np = dummy_input.cpu().numpy()
print(f"Dummy input numpy array created with shape: {dummy_input_np.shape}")

# 추론 실행
try:
    print("Running inference...")
    outputs = ort_session.run(None, {'input': dummy_input_np})
    print("Inference completed. Outputs:")
    print(outputs)
except Exception as e:
    print("Error running inference:", str(e))

2. CMakeLists.txt
cmake_minimum_required(VERSION 3.14)
project(Project1 LANGUAGES CXX CUDA)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# Vcpkg toolchain 파일 경로 설정
set(CMAKE_TOOLCHAIN_FILE "C:/Users/nayou/vcpkg/scripts/buildsystems/vcpkg.cmake" CACHE FILEPATH "Path to vcpkg toolchain file")

# Vcpkg 패키지가 설치된 경로 추가
set(CMAKE_PREFIX_PATH "C:/Users/nayou/vcpkg/installed/x64-windows" ${CMAKE_PREFIX_PATH})

# LibTorch 설정
set(TORCH_ROOT "C:/libtorch")
#set(TORCH_ROOT "C:/myLab/libtorch_Debug")  # Uncomment as needed
#set(TORCH_ROOT "C:/myLab/libtorch_Release")  # Uncomment as needed
set(Torch_DIR "${TORCH_ROOT}/share/cmake/Torch")

# Qt, OpenCV, CUDA, Torch 설정
set(CMAKE_PREFIX_PATH "C:/Qt/6.7.1/msvc2019_64" "C:/opencv/build")
find_package(Qt6 REQUIRED COMPONENTS Widgets Core Gui)
find_package(OpenCV REQUIRED COMPONENTS core imgproc highgui cudaarithm cudafilters cudawarping cudacodec cudafeatures2d cudaimgproc dnn)
find_package(CUDA REQUIRED)
find_package(GSL REQUIRED)
find_package(Torch REQUIRED)

# ONNX Runtime 설정
set(ONNX_RUNTIME_ROOT "C:/onnxruntime")
include_directories( 
    "${ONNX_RUNTIME_ROOT}"
    "${ONNX_RUNTIME_ROOT}/cmake/build/_deps/safeint-src"
    "${ONNX_RUNTIME_ROOT}/cmake/build/_deps/mp11-src/include"
    "${ONNX_RUNTIME_ROOT}/cmake/build"
    "${ONNX_RUNTIME_ROOT}/build"
    "${ONNX_RUNTIME_ROOT}/onnxruntime"
    "${ONNX_RUNTIME_ROOT}/build/_deps/mp11-src/include"
    "${ONNX_RUNTIME_ROOT}/build/_deps/safeint-src"
    "${ONNX_RUNTIME_ROOT}/include/onnxruntime"
    "${ONNX_RUNTIME_ROOT}/include/onnxruntime/core/session")
set(ONNX_RUNTIME_LIB_DIR "${ONNX_RUNTIME_ROOT}/build/${CMAKE_BUILD_TYPE}")

# IPP 설정
set(IPP_ROOT "C:/Program Files (x86)/Intel/oneAPI/ipp/2021.11")
include_directories("${IPP_ROOT}/include")
link_directories("${IPP_ROOT}/lib")

# GStreamer 설정
set(GSTREAMER_ROOT "C:/gstreamer/1.0/msvc_x86_64")
include_directories(
    "${GSTREAMER_ROOT}/include/gstreamer-1.0"
    "${GSTREAMER_ROOT}/include/glib-2.0"
    "${GSTREAMER_ROOT}/lib/glib-2.0/include"
)
link_directories("${GSTREAMER_ROOT}/lib")

# IPP 설정
set(IPLIB_ROOT "C:/myLab/Project1/Project1/imageProcessingLib")
include_directories("${IPLIB_ROOT}")
link_directories(
    "${IPLIB_ROOT}/build/lib/Debug"
    "${IPLIB_ROOT}/build/bin/Debug"
    "${IPLIB_ROOT}/build/Debug"
)

# Abseil 라이브러리 설정
set(ABSEIL_ROOT "C:/abseil-cpp")
include_directories("${ABSEIL_ROOT}")
link_directories("${ABSEIL_ROOT}/build/lib")

# 추가 포함 디렉터리 설정
include_directories(
    ${CUDA_INCLUDE_DIRS}
    ${OpenCV_INCLUDE_DIRS}
    "${CUDA_TOOLKIT_ROOT_DIR}/include"
    ${IPP_ROOT}/include
    "${GSTREAMER_ROOT}/include/gstreamer-1.0"
    "${GSTREAMER_ROOT}/lib/gstreamer-1.0/include"
    ${TORCH_INCLUDE_DIRS}  # LibTorch 포함 디렉토리
    ${ONNX_RUNTIME_INCLUDE_DIR}  # ONNX Runtime 포함 디렉토리
)

# GSL 설정 추가
include_directories(${GSL_INCLUDE_DIRS})
link_directories(${GSL_LIBRARY_DIR})

# CUDA 아키텍처 설정
set(CUDA_ARCHITECTURES "86")
message(STATUS "CUDA Architectures set to: ${CUDA_ARCHITECTURES}")

# CUDA 파일 설정
set(CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS} -std=c++14 --expt-relaxed-constexpr -gencode arch=compute_86,code=sm_86)

# CUDA 파일 컴파일 및 라이브러리 생성
cuda_add_library(image_processing
    imageProcessing.cu
)

# Qt 래핑 파일 생성
qt6_wrap_cpp(MOC_FILES
    MainWindow.h
    ImageProcessor.h
    DetectDialog.h
)

# 라이브러리 디렉토리 설정
link_directories("${CMAKE_BINARY_DIR}")

# 라이브러리 링크
target_link_libraries(image_processing
    ippcc.lib
    ippcore.lib
    ippvm.lib
    ipps.lib
    ippi.lib
    ippcv.lib
    nppial
    nppicc
    nppidei
    nppif
    nppig
    nppim
    nppist
    nppisu
    nppitc
    npps
    imageProcessingLib
    GSL::gsl
    GSL::gslcblas
)

# GStreamer 라이브러리 추가
list(APPEND GSTREAMER_LIBRARIES
    gstreamer-1.0.lib
    gobject-2.0.lib
    glib-2.0.lib
    gstapp-1.0.lib
    gstbase-1.0.lib
    gstvideo-1.0.lib
)

# 실행 파일 추가
add_executable(Project1
    main.cpp
    MainWindow.cpp
    MainWindow.h
    MainWindow.ui
    ImageProcessor.cpp
    ImageProcessor.h
    ${MOC_FILES}
)

# 라이브러리 링크
target_link_libraries(Project1
    Qt6::Widgets
    Qt6::Core
    Qt6::Gui
    ${OpenCV_LIBS}
    image_processing
    imageProcessingLib
    ${CUDA_LIBRARIES}
    ${CUDNN_LIBRARIES}
    ${GSTREAMER_LIBRARIES}
    "${TORCH_LIBRARIES}"  # LibTorch 링크
    ${ONNX_RUNTIME_LIBRARIES}  # ONNX Runtime 링크
    GSL::gsl
    GSL::gslcblas
)

# 빌드 후 TARGET_FILE_DIR 출력
add_custom_command(TARGET Project1 POST_BUILD
    COMMAND ${CMAKE_COMMAND} -E echo "TARGET_FILE_DIR for Project1: $<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E echo "$<TARGET_FILE_DIR:Project1>" > "Project1_dir.txt"
)

# 실행 파일 출력 디렉토리 설정
set_target_properties(Project1 PROPERTIES
    RUNTIME_OUTPUT_DIRECTORY_DEBUG "${CMAKE_BINARY_DIR}/Debug"
    RUNTIME_OUTPUT_DIRECTORY_RELEASE "${CMAKE_BINARY_DIR}/Release"
)

file(COPY "C:/myLab/Project1/Project1/imageProcessingLib/build/Debug/imageProcessingLib.dll"
     DESTINATION "${CMAKE_BINARY_DIR}/Debug"
)

# 디버그 빌드에서의 OpenCV opencv_world DLL 복사
add_custom_command(TARGET Project1 POST_BUILD
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/x64/vc16/bin/opencv_world4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMENT "Copying OpenCV Debug opencv_world DLL to output directory"
)

# 릴리스 빌드에서의 OpenCV opencv_world DLL 복사
add_custom_command(TARGET Project1 POST_BUILD
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/x64/vc16/bin/opencv_world4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMENT "Copying OpenCV Release opencv_world DLL to output directory"
)

# 디버그 빌드에서의 OpenCV DLL 복사
add_custom_command(TARGET Project1 POST_BUILD
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_dnn4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_cudaarithm4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_cudaimgproc4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_cudafilters4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_imgcodecs4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_core4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_imgproc4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMENT "Copying OpenCV Debug DLLs to output directory"
)

# 릴리스 빌드에서의 OpenCV DLL 복사
add_custom_command(TARGET Project1 POST_BUILD
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_dnn4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_cudaarithm4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_cudaimgproc4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_cudafilters4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_imgcodecs4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_core4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_imgproc4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMENT "Copying OpenCV Release DLLs to output directory"
)

# 파일 인코딩 설정 추가
add_compile_options("$<$<CXX_COMPILER_ID:MSVC>:/utf-8>")

# OpenMP 설정
find_package(OpenMP REQUIRED)
if(OpenMP_CXX_FOUND)
    target_link_libraries(Project1 OpenMP::OpenMP_CXX)
endif()

if (MSVC)
    set_property(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY VS_STARTUP_PROJECT Project1)

    # Debug 빌드
    set(CMAKE_CXX_FLAGS_DEBUG "${CMAKE_CXX_FLAGS_DEBUG} /MDd")
    set(CMAKE_C_FLAGS_DEBUG "${CMAKE_C_FLAGS_DEBUG} /MDd")

    # Release 빌드
    set(CMAKE_CXX_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE} /MD")
    set(CMAKE_C_FLAGS_RELEASE "${CMAKE_C_FLAGS_RELEASE} /MD")

    # 반복자 디버그 레벨 설정
    add_compile_definitions(
        $<$<CONFIG:Debug>:_ITERATOR_DEBUG_LEVEL=2>
        $<$<CONFIG:Release>:_ITERATOR_DEBUG_LEVEL=0>
    )
endif()

3. 감지프로그램 구현 및 테스트(OpenCV만)
//DetectDialog.h
#ifndef DETECTDIALOG_H
#define DETECTDIALOG_H

#include <iostream>
#include <vector>
#include <string>

#include <QImage>
#include <QPixmap>
#include <QLabel>
#include <QDebug>

#include <opencv2/dnn.hpp>
#include <opencv2/imgproc.hpp>
#include <opencv2/highgui.hpp>
#include <opencv2/core.hpp>

#include "ui_detect_result.h"

class DetectDialog : public QDialog
{
    Q_OBJECT

public:
    DetectDialog(QWidget* parent = nullptr);
    ~DetectDialog();

    void setImages(const cv::Mat imageOpenCV
                    , const cv::Mat imageIPP
                    , const cv::Mat imageNPP
                    , const cv::Mat imageCUDA
                    , const cv::Mat imageCUDAKernel
                    , const cv::Mat imageGStreamer);

private slots:
    void RunOnnx();

private:
    Ui::DetectDialog* ui;

    cv::Mat currentImageOpenCV;
    cv::Mat currentImageIPP;
    cv::Mat currentImageCUDA;
    cv::Mat currentImageCUDAKernel;
    cv::Mat currentImageNPP;
    cv::Mat currentImageGStreamer;

    cv::dnn::Net net; //OpenCV DNN 네트워크 객체

    void connectActions();
    void displayImage(cv::Mat image, QLabel* label);
    std::string getClassName() const;
};

#endif // DETECTDIALOG_H

//DetectDialog.cpp
#include "DetectDialog.h"

DetectDialog::DetectDialog(QWidget* parent)
    : QDialog(parent), ui(new Ui::DetectDialog)
{
    ui->setupUi(this);

    ui->label_opencv_time->setText(QString("OpenCV"));
    ui->label_ipp_time->setText(QString("IPP"));
    ui->label_npp_time->setText(QString("NPP"));
    ui->label_cuda_time->setText(QString("CUDA"));
    ui->label_cudakernel_time->setText(QString("CUDA Kernel"));
    ui->label_npp_time->setText(QString("NPP"));
    ui->label_gstreamer_time->setText(QString("GStreamer"));

    connectActions();
}

DetectDialog::~DetectDialog()
{
    delete ui;
}

void DetectDialog::connectActions()
{
    connect(ui->actionONNX, &QAction::triggered, this, &DetectDialog::RunOnnx);
}

void DetectDialog::setImages(const cv::Mat imageOpenCV, const cv::Mat imageIPP
    , const cv::Mat imageNPP, const cv::Mat imageCUDA, const cv::Mat imageCUDAKernel
    , const cv::Mat imageGStreamer)
{
    currentImageOpenCV = imageOpenCV;
    currentImageIPP = imageIPP;
    currentImageNPP = imageNPP;
    currentImageCUDA = imageCUDA;
    currentImageCUDAKernel = imageCUDAKernel;
    currentImageGStreamer = imageGStreamer;

    displayImage(currentImageOpenCV, ui->label_opencv);
    displayImage(currentImageIPP, ui->label_ipp);
    displayImage(currentImageNPP, ui->label_npp);
    displayImage(currentImageCUDA, ui->label_cuda);
    displayImage(currentImageCUDAKernel, ui->label_cudakernel);
    displayImage(currentImageGStreamer, ui->label_gstreamer);
}

void DetectDialog::displayImage(cv::Mat image, QLabel* label)
{
    // 이미지 타입에 따라 QImage를 생성합니다.
    QImage qImage;

    qDebug() << "displayImage() called with image type:" << image.type();

    // OpenCV의 Mat 이미지 타입에 따라 다른 QImage 형식을 사용합니다.
    if (image.type() == CV_8UC1) {
        qDebug() << "displayImage() type: grayscale CV_8UC1 Format_Grayscale8";
        qImage = QImage(image.data,
            image.cols,
            image.rows,
            static_cast<int>(image.step),
            QImage::Format_Grayscale8);
    }
    else if (image.type() == CV_8UC3) {
        qDebug() << ">>displayImage() type: BGR CV_8UC3 Format_RGB888";
        qImage = QImage(image.data,
            image.cols,
            image.rows,
            static_cast<int>(image.step),
            QImage::Format_RGB888).rgbSwapped(); // BGR -> RGB 순서로 변환
    }
    else if (image.type() == CV_8UC4) {
        qDebug() << "displayImage() type: BGRA CV_8UC4 Format_RGBA8888";
        qImage = QImage(image.data,
            image.cols,
            image.rows,
            static_cast<int>(image.step),
            QImage::Format_RGBA8888);
    }
    else if (image.type() == CV_16UC3) {
        qDebug() << "displayImage() type: BGR CV_16UC3 Format_RGB16";

        // 16-bit 이미지를 8-bit로 변환
        cv::Mat temp;
        image.convertTo(temp, CV_8UC3, 1.0 / 256.0);
        qImage = QImage(temp.data,
            temp.cols,
            temp.rows,
            static_cast<int>(temp.step),
            QImage::Format_RGB888).rgbSwapped(); // BGR -> RGB 순서로 변환
    }
    else if (image.type() == CV_16SC1) {
        qDebug() << "displayImage() type: 16-bit signed integer CV_16SC1 Format_Grayscale16";
        qImage = QImage(reinterpret_cast<const uchar*>(image.data),
            image.cols,
            image.rows,
            static_cast<int>(image.step),
            QImage::Format_Grayscale16);
    }
    else if (image.type() == CV_16SC3) {
        qDebug() << "displayImage() type: 16-bit signed integer CV_16SC3 Format_RGB16";
        qImage = QImage(reinterpret_cast<const uchar*>(image.data),
            image.cols,
            image.rows,
            static_cast<int>(image.step),
            QImage::Format_RGB16);
    }
    else {
        qDebug() << "displayImage() type: " << image.type() << " not supported";
        return; // 지원하지 않는 이미지 타입은 처리하지 않음
    }

    // QLabel 위젯에 QPixmap으로 이미지를 설정합니다.
    QPixmap pixmap = QPixmap::fromImage(qImage);
    label->setPixmap(pixmap);
    label->setScaledContents(false); // 이미지를 Label 크기에 맞게 조정
    label->adjustSize(); // Label 크기 조정
    qDebug() << "displayImage() finished";
}

std::string DetectDialog::getClassName() const
{
    return "DetectDialog";
}

void DetectDialog::RunOnnx() {
    std::cout << "<<<" << getClassName() << "::" << __func__ << ">>>" << std::endl;

    // ONNX 모델 파일 경로
    std::string modelPath = "C:/myLab/Project1/Project1/python/best.onnx";
    std::cout << "Loading ONNX model from: " << modelPath << std::endl;

    // 모델 로드
    net = cv::dnn::readNetFromONNX(modelPath);
    if (net.empty()) {
        std::cerr << "Failed to load ONNX model from " << modelPath << std::endl;
        qDebug() << "Failed to load ONNX model";
        ui->label_opencv_time->setText("Failed to load ONNX model");
        return;
    }
    std::cout << "ONNX model loaded successfully." << std::endl;

    // 이미지가 그레이스케일인 경우 3채널로 변환
    cv::Mat colorImage;
    if (currentImageOpenCV.channels() == 1) {
        std::cout << "Converting grayscale image to BGR..." << std::endl;
        cv::cvtColor(currentImageOpenCV, colorImage, cv::COLOR_GRAY2BGR);
    }
    else {
        colorImage = currentImageOpenCV;
    }

    std::cout << "Image size: " << colorImage.cols << "x" << colorImage.rows << std::endl;

    // 이미지 전처리: 모델의 입력 크기에 맞추어 크기 조정
    cv::Mat resizedImage;
    cv::resize(colorImage, resizedImage, cv::Size(640, 640));  // 모델의 입력 크기
    std::cout << "Resized image to: " << resizedImage.cols << "x" << resizedImage.rows << std::endl;

    cv::Mat blob;
    cv::dnn::blobFromImage(resizedImage, blob, 1.0 / 255.0, cv::Size(640, 640), cv::Scalar(0, 0, 0), true, false);
    std::cout << "Blob created with size: " << blob.size << std::endl;

    // 네트워크에 입력 전달
    net.setInput(blob);
    std::cout << "Performing forward pass..." << std::endl;
    cv::Mat output = net.forward();
    std::cout << "Forward pass completed." << std::endl;

    // 출력 처리
    std::vector<cv::Mat> outputs;
    outputs.push_back(output);

    const int numClasses = 80; // 클래스 수
    const int numAnchors = 3; // 앵커 수
    const int gridSize = 20; // 그리드 크기
    const float confThreshold = 0.5; // confidence threshold
    const float nmsThreshold = 0.4; // non-maximum suppression threshold

    std::cout << "Processing detection output..." << std::endl;
    cv::Mat detection = outputs[0];
    std::vector<int> classIds;
    std::vector<float> confidences;
    std::vector<cv::Rect> boxes;

    // 원본 이미지 크기와 비율
    float scaleX = static_cast<float>(currentImageOpenCV.cols) / 640.0f;
    float scaleY = static_cast<float>(currentImageOpenCV.rows) / 640.0f;

    for (int batch = 0; batch < detection.size[0]; ++batch) {
        for (int anchor = 0; anchor < numAnchors; ++anchor) {
            for (int y = 0; y < detection.size[2]; ++y) {
                for (int x = 0; x < detection.size[3]; ++x) {
                    float* data = reinterpret_cast<float*>(detection.data + detection.step[1] * anchor * (numClasses + 5) + detection.step[2] * y + detection.step[3] * x);

                    float x_center = (data[0] + x) * resizedImage.cols / gridSize;
                    float y_center = (data[1] + y) * resizedImage.rows / gridSize;
                    float w = exp(data[2]) * resizedImage.cols / gridSize;
                    float h = exp(data[3]) * resizedImage.rows / gridSize;
                    float conf = data[4];

                    if (conf > confThreshold) {
                        float* classScores = data + 5;
                        int classId = std::max_element(classScores, classScores + numClasses) - classScores;
                        float classScore = classScores[classId];

                        if (classScore > confThreshold) {
                            int x1 = static_cast<int>(std::max(0.0f, (x_center - w / 2) * scaleX));
                            int y1 = static_cast<int>(std::max(0.0f, (y_center - h / 2) * scaleY));
                            int width = static_cast<int>(std::max(0.0f, w * scaleX));
                            int height = static_cast<int>(std::max(0.0f, h * scaleY));

                            // 바운딩 박스가 원본 이미지 크기를 초과하지 않도록 조정
                            if (x1 + width > currentImageOpenCV.cols) width = currentImageOpenCV.cols - x1;
                            if (y1 + height > currentImageOpenCV.rows) height = currentImageOpenCV.rows - y1;

                            if (width > 0 && height > 0) {
                                classIds.push_back(classId);
                                confidences.push_back(conf * classScore);
                                boxes.push_back(cv::Rect(x1, y1, width, height));
                            }
                        }
                    }
                }
            }
        }
    }
    std::cout << "Detection completed. Found " << boxes.size() << " boxes." << std::endl;

    // Non-maximum suppression
    std::vector<int> indices;
    cv::dnn::NMSBoxes(boxes, confidences, confThreshold, nmsThreshold, indices);

    // 결과 시각화
    if (indices.empty()) {
        std::cout << "No detections found." << std::endl;
        ui->label_opencv_time->setText("Recognition failed!");
    }
    else {
        std::cout << "Drawing bounding boxes..." << std::endl;
        for (size_t i = 0; i < indices.size(); ++i) {
            int idx = indices[i];
            cv::Rect box = boxes[idx];
            cv::rectangle(colorImage, box, cv::Scalar(0, 255, 0), 2);
            std::string label = cv::format("Class %d: %.2f", classIds[idx], confidences[idx]);
            cv::putText(colorImage, label, cv::Point(box.x, box.y - 10), cv::FONT_HERSHEY_SIMPLEX, 0.5, cv::Scalar(0, 255, 0), 2);
        }

        // 처리된 이미지 표시
        displayImage(colorImage, ui->label_opencv);
        ui->label_opencv_time->setText("Recognition complete!");
        std::cout << "Recognition complete!" << std::endl;
    }
}
