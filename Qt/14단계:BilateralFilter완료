<수정사항>
1. BilateralFilter 구현완료
OpenCV, IPP, NPP, CUDA, CUDAKernel, GStreamer
2. DLL, Lib 복사 구현
CMakeLists.txt수정

<코드>
1. imageProcessorNPP.cpp
#include "ImageProcessorNPP.h"

ImageProcessorNPP::ImageProcessorNPP()
{
}

ImageProcessorNPP::~ImageProcessorNPP()
{
}

cv::Mat ImageProcessorNPP::grayScale(cv::Mat& inputImage) {
    if (inputImage.channels() != 3) {
        std::cerr << "입력 이미지는 컬러 이미지여야 합니다." << std::endl;
        return cv::Mat();
    }

    // CUDA 스트림 생성
    cudaStream_t cudaStream;

    cudaStreamCreate(&cudaStream);

    // CUDA 메모리 할당
    Npp8u* d_src = nullptr;
    Npp8u* d_dst = nullptr;

    cudaMalloc(&d_src, inputImage.total() * inputImage.elemSize());
    cudaMalloc(&d_dst, inputImage.total() * sizeof(Npp8u)); // 그레이스케일 이미지: 1 채널

    cudaMemcpy(d_src, inputImage.data, inputImage.total() * inputImage.elemSize(), cudaMemcpyHostToDevice);

    // NPP 계수 설정 (RGB를 그레이스케일로 변환하기 위한 기본 계수)
    Npp32f aCoeffs[3] = { 0.299f, 0.587f, 0.114f };

    // NPP 스트림 컨텍스트 생성 및 초기화
    NppStreamContext nppStreamCtx;
    nppStreamCtx.hStream = cudaStream;
    // nppStream 필드는 최신 NPP 라이브러리에서는 사용되지 않음

    // NPP 그레이스케일 변환 함수 호출
    NppiSize oSizeROI = { inputImage.cols, inputImage.rows };
    NppStatus status = nppiColorToGray_8u_C3C1R_Ctx(d_src, inputImage.cols * inputImage.elemSize(),
        d_dst, inputImage.cols,
        oSizeROI, aCoeffs, nppStreamCtx);

    if (status != NPP_SUCCESS) {
        std::cerr << "NPP 함수 호출 실패: " << status << std::endl;
        cudaFree(d_src);
        cudaFree(d_dst);
        cudaStreamDestroy(cudaStream);
        return cv::Mat();
    }

    // CUDA 스트림 동기화
    cudaStreamSynchronize(cudaStream);

    // 결과를 호스트 메모리로 복사
    cv::Mat outputImage(inputImage.rows, inputImage.cols, CV_8UC1);
    cudaMemcpy(outputImage.data, d_dst, outputImage.total() * outputImage.elemSize(), cudaMemcpyDeviceToHost);

    // CUDA 메모리 해제
    cudaFree(d_src);
    cudaFree(d_dst);
    cudaStreamDestroy(cudaStream);

    return outputImage;
}

cv::Mat ImageProcessorNPP::rotate(cv::Mat& inputImage, bool isRight) {
    if (inputImage.empty()) {
        std::cerr << "Input image is empty." << std::endl;
        return cv::Mat();
    }

    int srcWidth = inputImage.cols;
    int srcHeight = inputImage.rows;
    int numChannels = inputImage.channels();

    // 회전 각도 설정
    double angleInDegrees = isRight ? 90.0 : -90.0;
    double angleInRadians = angleInDegrees * CV_PI / 180.0; // 각도를 라디안 단위로 변환

    double cosAngle = std::abs(std::cos(angleInRadians));
    double sinAngle = std::abs(std::sin(angleInRadians));
    int dstWidth = static_cast<int>(srcWidth * cosAngle + srcHeight * sinAngle);
    int dstHeight = static_cast<int>(srcWidth * sinAngle + srcHeight * cosAngle);

    // 회전된 이미지를 위한 Mat 생성
    cv::Mat outputImage(dstHeight, dstWidth, inputImage.type());

    // GPU 메모리로 이미지 업로드
    cv::cuda::GpuMat d_inputImage, d_outputImage;
    d_inputImage.upload(inputImage);
    d_outputImage.create(dstHeight, dstWidth, inputImage.type());

    // CUDA 스트림 생성
    cudaStream_t stream;
    cudaStreamCreate(&stream);

    // NPP 함수 호출을 위한 포인터
    Npp8u* pSrc = d_inputImage.ptr<Npp8u>();
    Npp8u* pDst = d_outputImage.ptr<Npp8u>();

    // 이미지 크기 및 스트라이드
    NppiSize oSrcSize = { srcWidth, srcHeight };
    NppiSize oDstSize = { dstWidth, dstHeight };
    int srcStep = d_inputImage.step1();  // 입력 이미지의 행 간격
    int dstStep = d_outputImage.step1(); // 출력 이미지의 행 간격

    // 회전 중심 설정 (이미지의 중심)
    double centerX = (srcWidth - 1) / 2.0;
    double centerY = (srcHeight - 1) / 2.0;

    // 출력 이미지의 ROI를 설정
    NppiRect oSrcROI = { 0, 0, srcWidth, srcHeight };
    NppiRect oDstROI = { 0, 0, dstWidth, dstHeight };

    NppStatus nppStatus;
    if (numChannels == 3) {
        // 컬러 이미지 회전
        nppStatus = nppiRotate_8u_C3R(
            pSrc, oSrcSize, srcStep, oSrcROI,
            pDst, dstStep, oDstROI,
            angleInDegrees, centerX, centerY, NPPI_INTER_LINEAR
        );
    }
    else if (numChannels == 1) {
        // 그레이스케일 이미지 회전
        nppStatus = nppiRotate_8u_C1R(
            pSrc, oSrcSize, srcStep, oSrcROI,
            pDst, dstStep, oDstROI,
            angleInDegrees, centerX, centerY, NPPI_INTER_LINEAR
        );
    }
    else {
        std::cerr << "Unsupported image format." << std::endl;
        return cv::Mat();
    }

    // NPP 오류 처리
    checkNPPError(nppStatus);

    // GPU 이미지를 CPU 메모리로 다운로드
    d_outputImage.download(outputImage);

    // CUDA 스트림 파괴
    cudaStreamDestroy(stream);

    return outputImage;
}

cv::Mat ImageProcessorNPP::zoom(cv::Mat& inputImage, double newWidth, double newHeight) {
    
    int inputWidth = inputImage.cols;
    int inputHeight = inputImage.rows;
    int newWidthInt = static_cast<int>(newWidth);
    int newHeightInt = static_cast<int>(newHeight);

    // 디버깅: 입력 이미지 크기 출력
    std::cout << "Input Image Size: " << inputWidth << "x" << inputHeight << std::endl;
    std::cout << "New Image Size: " << newWidthInt << "x" << newHeightInt << std::endl;

    // GPU 메모리로 이미지 업로드
    cv::cuda::GpuMat d_inputImage;
    d_inputImage.upload(inputImage);

    // GPU 메모리에 출력 이미지를 위한 GpuMat
    cv::cuda::GpuMat d_outputImage(newHeightInt, newWidthInt, inputImage.type());

    // NPP 함수 호출을 위한 포인터
    Npp8u* pSrc = d_inputImage.ptr<Npp8u>();
    Npp8u* pDst = d_outputImage.ptr<Npp8u>();

    // 이미지 크기 및 스트라이드
    NppiSize oSrcSize = { inputWidth, inputHeight };
    NppiSize oDstSize = { newWidthInt, newHeightInt };
    int srcStep = d_inputImage.step;  // 입력 이미지의 행 간격
    int dstStep = d_outputImage.step; // 출력 이미지의 행 간격

    // 디버깅: 스트라이드 값 출력
    std::cout << "Source Step: " << srcStep << std::endl;
    std::cout << "Destination Step: " << dstStep << std::endl;

    // 입력 및 출력 ROI 설정
    NppiRect oSrcRectROI = { 0, 0, inputWidth, inputHeight };
    NppiRect oDstRectROI = { 0, 0, newWidthInt, newHeightInt };

    // 디버깅: ROI 값 출력
    std::cout << "Source ROI: " << oSrcRectROI.x << ", " << oSrcRectROI.y << ", " << oSrcRectROI.width << ", " << oSrcRectROI.height << std::endl;
    std::cout << "Destination ROI: " << oDstRectROI.x << ", " << oDstRectROI.y << ", " << oDstRectROI.width << ", " << oDstRectROI.height << std::endl;

    NppStatus nppStatus;

    if (inputImage.channels() == 3) {
        // NPP 컬러 이미지 리사이즈 함수 호출
        nppStatus = nppiResize_8u_C3R(
            pSrc, srcStep,         // 입력 이미지의 stride
            oSrcSize,              // 입력 이미지 크기
            oSrcRectROI,           // 입력 이미지 영역
            pDst, dstStep,         // 출력 이미지의 stride
            oDstSize,              // 출력 이미지 크기
            oDstRectROI,           // 출력 이미지 영역
            NPPI_INTER_LINEAR      // 보간법 (선형 보간법)
        );
    }
    else if (inputImage.channels() == 1) {
        // NPP 그레이스케일 이미지 리사이즈 함수 호출
        nppStatus = nppiResize_8u_C1R(
            pSrc, srcStep,         // 입력 이미지의 stride
            oSrcSize,              // 입력 이미지 크기
            oSrcRectROI,           // 입력 이미지 영역
            pDst, dstStep,         // 출력 이미지의 stride
            oDstSize,              // 출력 이미지 크기
            oDstRectROI,           // 출력 이미지 영역
            NPPI_INTER_LINEAR      // 보간법 (선형 보간법)
        );
    }
    else {
        std::cerr << "Unsupported image format." << std::endl;
        return cv::Mat();
    }

    // NPP 오류 처리
    checkNPPError(nppStatus);

    // GPU 이미지를 CPU 메모리로 다운로드
    cv::Mat outputImage(newHeightInt, newWidthInt, inputImage.type());
    d_outputImage.download(outputImage);

    return outputImage;
}



cv::Mat ImageProcessorNPP::gaussianBlur(cv::Mat& inputImage, int kernelSize) {
    if (kernelSize % 2 == 0 || kernelSize < 3) {
        std::cerr << "Kernel size must be an odd number and greater than or equal to 3." << std::endl;
        return cv::Mat();
    }

    std::cout << "Kernel size: " << kernelSize << std::endl;
    std::cout << "Input image size: " << inputImage.cols << " x " << inputImage.rows << std::endl;
    std::cout << "Input image type: " << cv::typeToString(inputImage.type()) << std::endl;

    cv::Mat outputImage(inputImage.size(), inputImage.type());

    void* d_input = nullptr;
    void* d_output = nullptr;
    cudaError_t cudaStatus;

    cudaStatus = cudaMalloc(&d_input, inputImage.rows * inputImage.cols * inputImage.elemSize());
    std::cout << "cudaMalloc for d_input: " << cudaGetErrorString(cudaStatus) << std::endl;
    std::cout << "d_input address: " << d_input << std::endl;
    if (cudaStatus != cudaSuccess) {
        std::cerr << "Failed to allocate GPU memory for d_input. Error code: " << cudaStatus << std::endl;
        return cv::Mat();
    }

    cudaStatus = cudaMalloc(&d_output, inputImage.rows * inputImage.cols * inputImage.elemSize());
    std::cout << "cudaMalloc for d_output: " << cudaGetErrorString(cudaStatus) << std::endl;
    std::cout << "d_output address: " << d_output << std::endl;
    if (cudaStatus != cudaSuccess) {
        std::cerr << "Failed to allocate GPU memory for d_output. Error code: " << cudaStatus << std::endl;
        cudaFree(d_input);
        return cv::Mat();
    }

    cudaStatus = cudaMemcpy(d_input, inputImage.data, inputImage.rows * inputImage.cols * inputImage.elemSize(), cudaMemcpyHostToDevice);
    std::cout << "cudaMemcpy HostToDevice: " << cudaGetErrorString(cudaStatus) << std::endl;
    if (cudaStatus != cudaSuccess) {
        std::cerr << "Failed to copy data to GPU. Error code: " << cudaStatus << std::endl;
        cudaFree(d_input);
        cudaFree(d_output);
        return cv::Mat();
    }

    NppiSize oSizeROI = { inputImage.cols, inputImage.rows };
    NppiMaskSize maskSize;

    switch (kernelSize) {
    case 3: maskSize = NPP_MASK_SIZE_3_X_3; break;
    case 5: maskSize = NPP_MASK_SIZE_5_X_5; break;
    case 7: maskSize = NPP_MASK_SIZE_7_X_7; break;
    default:
        std::cerr << "Unsupported mask size. Only 3, 5, and 7 are supported." << std::endl;
        cudaFree(d_input);
        cudaFree(d_output);
        return cv::Mat();
    }

    int srcStep = inputImage.cols * inputImage.elemSize();
    int dstStep = inputImage.cols * inputImage.elemSize();

    std::cout << "NPP Filter Gaussian parameters: " << std::endl;
    std::cout << "Source Step (stride): " << srcStep << std::endl;
    std::cout << "Destination Step (stride): " << dstStep << std::endl;
    std::cout << "ROI Size: " << oSizeROI.width << " x " << oSizeROI.height << std::endl;
    std::cout << "Mask Size: " << kernelSize << std::endl;

    // Check image type and select appropriate NPP function
    NppStatus status;
    switch (inputImage.type()) {
    case CV_8UC1: {
        status = nppiFilterGauss_8u_C1R(
            static_cast<Npp8u*>(d_input),
            inputImage.cols * inputImage.elemSize(),
            static_cast<Npp8u*>(d_output),
            inputImage.cols * inputImage.elemSize(),
            oSizeROI,
            maskSize
        );
        break;
    }
    case CV_16UC1: {
        status = nppiFilterGauss_16u_C1R(
            static_cast<Npp16u*>(d_input),
            inputImage.cols * inputImage.elemSize(),
            static_cast<Npp16u*>(d_output),
            inputImage.cols * inputImage.elemSize(),
            oSizeROI,
            maskSize
        );
        break;
    }
    case CV_16SC1: {
        status = nppiFilterGauss_16s_C1R(
            static_cast<Npp16s*>(d_input),
            inputImage.cols * inputImage.elemSize(),
            static_cast<Npp16s*>(d_output),
            inputImage.cols * inputImage.elemSize(),
            oSizeROI,
            maskSize
        );
        break;
    }
    case CV_8UC3: { // 16
        status = nppiFilterGauss_8u_C3R(
            static_cast<Npp8u*>(d_input),
            inputImage.cols * inputImage.elemSize(),
            static_cast<Npp8u*>(d_output),
            inputImage.cols * inputImage.elemSize(),
            oSizeROI,
            maskSize
        );
        break;
    }
    case CV_16UC3: {
        status = nppiFilterGauss_16u_C3R(
            static_cast<Npp16u*>(d_input),
            inputImage.cols * inputImage.elemSize(),
            static_cast<Npp16u*>(d_output),
            inputImage.cols * inputImage.elemSize(),
            oSizeROI,
            maskSize
        );
        break;
    }
    case CV_16SC3: {
        status = nppiFilterGauss_16s_C3R(
            static_cast<Npp16s*>(d_input),
            inputImage.cols * inputImage.elemSize(),
            static_cast<Npp16s*>(d_output),
            inputImage.cols * inputImage.elemSize(),
            oSizeROI,
            maskSize
        );
        break;
    }
    default: {
        std::cerr << "Unsupported image type. Type code: " << inputImage.type() << std::endl;
        cudaFree(d_input);
        cudaFree(d_output);
        return cv::Mat();
    }
    }


    std::cout << "nppiFilterGauss status: " << status << std::endl;
    if (status != NPP_SUCCESS) {
        std::cerr << "Failed to apply NPP Gaussian filter. Error code: " << status << std::endl;
        cudaFree(d_input);
        cudaFree(d_output);
        return cv::Mat();
    }

    cudaStatus = cudaMemcpy(outputImage.data, d_output, inputImage.rows * inputImage.cols * inputImage.elemSize(), cudaMemcpyDeviceToHost);
    std::cout << "cudaMemcpy DeviceToHost: " << cudaGetErrorString(cudaStatus) << std::endl;
    if (cudaStatus != cudaSuccess) {
        std::cerr << "Failed to copy result back to host. Error code: " << cudaStatus << std::endl;
        cudaFree(d_input);
        cudaFree(d_output);
        return cv::Mat();
    }

    cudaFree(d_input);
    cudaFree(d_output);

    return outputImage;
}




cv::Mat ImageProcessorNPP::cannyEdges(cv::Mat& inputImage) {

    cv::Mat grayImage;
    cv::Mat edgeImage;
    cv::Mat outputImage;

    // Convert to grayscale if necessary
    if (inputImage.channels() == 3) {
        //cv::cvtColor(inputImage, grayImage, cv::COLOR_BGR2GRAY);
        grayImage = grayScale(inputImage);
    }
    else {
        grayImage = inputImage.clone();
    }

    // Prepare parameters for NPP function
    NppiSize oSrcSize = { grayImage.cols, grayImage.rows };
    NppiSize oSizeROI = { grayImage.cols, grayImage.rows };
    NppiPoint oSrcOffset = { 0, 0 };
    int srcStep = static_cast<int>(grayImage.step);
    int dstStep = srcStep;

    // Allocate GPU memory and upload data
    cv::cuda::GpuMat d_src, d_dst;
    d_src.upload(grayImage);
    d_dst.create(grayImage.size(), CV_8UC1);

    // Create NPP buffer
    int bufferSize = 0;
    NppStatus status = nppiFilterCannyBorderGetBufferSize(oSizeROI, &bufferSize);
    checkNPPStatus(status, "nppiFilterCannyBorderGetBufferSize");

    // Ensure that buffer size is valid
    if (bufferSize <= 0) {
        std::cerr << "Invalid buffer size: " << bufferSize << std::endl;
        return cv::Mat();
    }

    cv::cuda::GpuMat nppBuffer;
    nppBuffer.create(1, bufferSize, CV_8UC1);
    Npp8u* pBuffer = nppBuffer.ptr<Npp8u>();

    Npp8u* pSrcData = d_src.ptr<Npp8u>();
    Npp8u* pDstData = d_dst.ptr<Npp8u>();

    // Call the NPP function
    status = nppiFilterCannyBorder_8u_C1R(
        pSrcData, srcStep, oSrcSize, oSrcOffset,
        pDstData, dstStep, oSizeROI,
        NPP_FILTER_SOBEL, NPP_MASK_SIZE_3_X_3,
        50, 150, NppiNorm(nppiNormL2),
        NPP_BORDER_REPLICATE, pBuffer
    );
    checkNPPStatus(status, "nppiFilterCannyBorder_8u_C1R");

    // Download result from GPU
    d_dst.download(edgeImage);

    // Ensure edgeImage is correctly processed
    if (edgeImage.empty()) {
        std::cerr << "Edge image is empty after download." << std::endl;
        return cv::Mat();
    }

    if (inputImage.channels() == 3) {
        // 컬러 이미지 처리
        outputImage = inputImage.clone();
        for (int y = 0; y < edgeImage.rows; ++y) {
            for (int x = 0; x < edgeImage.cols; ++x) {
                if (edgeImage.at<uchar>(y, x) > 0) {
                    outputImage.at<cv::Vec3b>(y, x) = cv::Vec3b(0, 255, 0); // 초록색
                }
            }
        }
    }
    else {
        // 흑백 이미지 처리
        outputImage = cv::Mat(grayImage.size(), CV_8UC1, cv::Scalar(0));
        for (int y = 0; y < edgeImage.rows; ++y) {
            for (int x = 0; x < edgeImage.cols; ++x) {
                if (edgeImage.at<uchar>(y, x) > 0) {
                    outputImage.at<uchar>(y, x) = 255; // 흰색
                }
            }
        }
    }

    return outputImage;
}

cv::Mat ImageProcessorNPP::medianFilter(cv::Mat& inputImage) {    

    if (inputImage.empty()) {
        std::cerr << "Input image is empty." << std::endl;
        return cv::Mat(); // 빈 이미지 반환
    }

    int numChannels = inputImage.channels();
    cv::Mat outputImage(inputImage.size(), inputImage.type());

    Npp32s nSrcStep = inputImage.step[0];
    Npp32s nDstStep = outputImage.step[0];
    NppiSize oSizeROI = { inputImage.cols, inputImage.rows };
    NppiSize oMaskSize = { 5, 5 }; // 5x5 필터
    NppiPoint oAnchor = { 2, 2 }; // 5x5 필터의 중앙

    try {
        NppStatus status;
        Npp32u nBufferSize = 0;

        // CUDA 초기화
        cudaSetDevice(0);        

        // 버퍼 메모리 할당 및 이미지 처리
        Npp8u* d_buffer;
        cudaMalloc(&d_buffer, nBufferSize);

        Npp8u* d_src;
        Npp8u* d_dst;
        size_t imageSize = inputImage.rows * inputImage.step;

        cudaMalloc(&d_src, imageSize);
        cudaMalloc(&d_dst, imageSize);

        // 입력 이미지를 GPU 메모리로 복사
        cudaMemcpy(d_src, inputImage.data, imageSize, cudaMemcpyHostToDevice);

        // 미디안 필터 적용
        if (numChannels == 3) {
            status = nppiFilterMedian_8u_C3R(
                d_src, nSrcStep, d_dst, nDstStep, oSizeROI, oMaskSize, oAnchor, d_buffer
            );

            std::cout << "nppiFilterMedian_8u_C3R status: " << status << std::endl;
            if (status != NPP_SUCCESS) {
                std::cerr << "nppiFilterMedian_8u_C3R failed with status: " << status << std::endl;
                throw std::runtime_error("nppiFilterMedian_8u_C3R failed.");
            }
        }
        else if (numChannels == 1) {
            status = nppiFilterMedian_8u_C1R(
                d_src, nSrcStep, d_dst, nDstStep, oSizeROI, oMaskSize, oAnchor, d_buffer
            );

            std::cout << "nppiFilterMedian_8u_C1R status: " << status << std::endl;
            if (status != NPP_SUCCESS) {
                std::cerr << "nppiFilterMedian_8u_C1R failed with status: " << status << std::endl;
                throw std::runtime_error("nppiFilterMedian_8u_C1R failed.");
            }
        }

        // 결과를 호스트로 복사
        cudaMemcpy(outputImage.data, d_dst, imageSize, cudaMemcpyDeviceToHost);

        // GPU 메모리 해제
        cudaFree(d_src);
        cudaFree(d_dst);
        cudaFree(d_buffer);

        // CUDA 에러 체크
        cudaError_t cudaErr = cudaGetLastError();
        if (cudaErr != cudaSuccess) {
            std::cerr << "CUDA error: " << cudaGetErrorString(cudaErr) << std::endl;
            throw std::runtime_error("CUDA error occurred.");
        }

    }
    catch (const std::exception& ex) {
        std::cerr << "Exception: " << ex.what() << std::endl;
        return cv::Mat(); // 빈 이미지 반환
    }

    return outputImage;
}

cv::Mat ImageProcessorNPP::sobelFilter(cv::Mat& inputImage) {
    if (inputImage.empty()) {
        std::cerr << "Input image is empty." << std::endl;
        return cv::Mat(); // 빈 이미지 반환
    }

    int numChannels = inputImage.channels();
    cv::Mat outputImage(inputImage.size(), inputImage.type());

    Npp32s nSrcStep = inputImage.step[0];
    Npp32s nDstStep = outputImage.step[0];
    NppiSize oSizeROI = { inputImage.cols, inputImage.rows };

    NppStatus status;

    // CUDA 초기화
    cudaSetDevice(0);

    size_t imageSize = inputImage.rows * inputImage.step;

    unsigned char* d_src;
    unsigned char* d_dst;

    // CUDA 메모리 할당
    cudaMalloc(&d_src, imageSize);
    cudaMalloc(&d_dst, imageSize);

    // 입력 이미지를 GPU 메모리로 복사
    cudaMemcpy(d_src, inputImage.data, imageSize, cudaMemcpyHostToDevice);

    // 소벨 필터 처리
    try {
        if (numChannels == 3) {
            // 수평 소벨 필터
            status = nppiFilterSobelHoriz_8u_C3R(d_src, nSrcStep, d_dst, nDstStep, oSizeROI);
            if (status != NPP_SUCCESS) {
                std::cerr << "nppiFilterSobelHoriz_8u_C3R failed with status: " << status << std::endl;
                throw std::runtime_error("nppiFilterSobelHoriz_8u_C3R failed.");
            }

            // 수직 소벨 필터
            status = nppiFilterSobelVert_8u_C3R(d_dst, nSrcStep, d_dst, nDstStep, oSizeROI);
            if (status != NPP_SUCCESS) {
                std::cerr << "nppiFilterSobelVert_8u_C3R failed with status: " << status << std::endl;
                throw std::runtime_error("nppiFilterSobelVert_8u_C3R failed.");
            }
        }
        else if (numChannels == 1) {
            // 그레이스케일 이미지에서 소벨 필터 적용

            // 수평 소벨 필터
            status = nppiFilterSobelHoriz_8u_C1R(d_src, nSrcStep, d_dst, nDstStep, oSizeROI);
            if (status != NPP_SUCCESS) {
                std::cerr << "nppiFilterSobelHoriz_8u_C1R failed with status: " << status << std::endl;
                throw std::runtime_error("nppiFilterSobelHoriz_8u_C1R failed.");
            }

            // 수직 소벨 필터
            status = nppiFilterSobelVert_8u_C1R(d_dst, nSrcStep, d_dst, nDstStep, oSizeROI);
            if (status != NPP_SUCCESS) {
                std::cerr << "nppiFilterSobelVert_8u_C1R failed with status: " << status << std::endl;
                throw std::runtime_error("nppiFilterSobelVert_8u_C1R failed.");
            }
        }
        else {
            std::cerr << "Unsupported number of channels: " << numChannels << std::endl;
            cudaFree(d_src);
            cudaFree(d_dst);
            return cv::Mat(); // 빈 이미지 반환
        }

        // 결과를 호스트로 복사
        cudaMemcpy(outputImage.data, d_dst, imageSize, cudaMemcpyDeviceToHost);

        // CUDA 오류 체크
        cudaError_t cudaErr = cudaGetLastError();
        if (cudaErr != cudaSuccess) {
            std::cerr << "CUDA error: " << cudaGetErrorString(cudaErr) << std::endl;
            throw std::runtime_error("CUDA error occurred.");
        }
    }
    catch (const std::exception& ex) {
        std::cerr << "Exception: " << ex.what() << std::endl;
        cudaFree(d_src);
        cudaFree(d_dst);
        return cv::Mat(); // 빈 이미지 반환
    }

    // GPU 메모리 해제
    cudaFree(d_src);
    cudaFree(d_dst);

    return outputImage;
}

cv::Mat ImageProcessorNPP::bilateralFilter(cv::Mat& inputImage)
{
    // 입력 이미지를 GPU 메모리로 복사
    Npp8u* d_inputImage;
    size_t inputImagePitch;
    cudaMallocPitch((void**)&d_inputImage, &inputImagePitch, inputImage.cols * sizeof(Npp8u) * inputImage.channels(), inputImage.rows);
    cudaMemcpy2D(d_inputImage, inputImagePitch, inputImage.data, inputImage.step, inputImage.cols * sizeof(Npp8u) * inputImage.channels(), inputImage.rows, cudaMemcpyHostToDevice);

    // 출력 이미지를 GPU 메모리로 할당
    cv::Mat outputImage(inputImage.size(), inputImage.type());
    Npp8u* d_outputImage;
    size_t outputImagePitch;
    cudaMallocPitch((void**)&d_outputImage, &outputImagePitch, outputImage.cols * sizeof(Npp8u) * outputImage.channels(), outputImage.rows);

    // 양방향 필터 파라미터 설정
    NppiSize oSrcSize = { inputImage.cols, inputImage.rows };
    NppiPoint oSrcOffset = { 0, 0 };
    Npp32f nValSquareSigma = 75.0f;
    Npp32f nPosSquareSigma = 75.0f;
    int nRadius = 9;
    NppiBorderType eBorderType = NPP_BORDER_REPLICATE;

    // 이미지 채널 수에 따라 함수 선택
    if (inputImage.channels() == 1) {
        NppStatus status = nppiFilterBilateralGaussBorder_8u_C1R(d_inputImage, static_cast<int>(inputImagePitch), oSrcSize, oSrcOffset,
            d_outputImage, static_cast<int>(outputImagePitch), oSrcSize,
            nRadius, 1, nValSquareSigma, nPosSquareSigma, eBorderType);
        if (status != NPP_SUCCESS) {
            std::cerr << "Error applying bilateral filter: " << status << std::endl;
        }
    }
    else if (inputImage.channels() == 3) {
        NppStatus status = nppiFilterBilateralGaussBorder_8u_C3R(d_inputImage, static_cast<int>(inputImagePitch), oSrcSize, oSrcOffset,
            d_outputImage, static_cast<int>(outputImagePitch), oSrcSize,
            nRadius, 3, nValSquareSigma, nPosSquareSigma, eBorderType);
        if (status != NPP_SUCCESS) {
            std::cerr << "Error applying bilateral filter: " << status << std::endl;
        }
    }
    else {
        std::cerr << "Unsupported number of channels: " << inputImage.channels() << std::endl;
    }

    // 처리된 이미지를 호스트로 복사
    cudaMemcpy2D(outputImage.data, outputImage.step, d_outputImage, outputImagePitch, outputImage.cols * sizeof(Npp8u) * outputImage.channels(), outputImage.rows, cudaMemcpyDeviceToHost);

    // 메모리 해제
    cudaFree(d_inputImage);
    cudaFree(d_outputImage);

    return outputImage;
}

2. imageProcessorGStreamer.cpp
#include "ImageProcessorGStreamer.h"

ImageProcessorGStreamer::ImageProcessorGStreamer()
{
}

ImageProcessorGStreamer::~ImageProcessorGStreamer()
{
}

cv::Mat ImageProcessorGStreamer::grayScale(cv::Mat& inputImage) {
    if (inputImage.empty()) {
        std::cerr << "Input image is empty." << std::endl;
        return cv::Mat(); // Return empty image
    }

    GstElement* pipeline = nullptr;
    GstElement* source = nullptr;
    GstElement* convert = nullptr;
    GstElement* sink = nullptr;
    GstSample* sample = nullptr;

    gst_init(nullptr, nullptr);

    // Create GStreamer elements
    pipeline = gst_pipeline_new("pipeline");
    source = gst_element_factory_make("appsrc", "source");
    convert = gst_element_factory_make("videoconvert", "convert");
    sink = gst_element_factory_make("appsink", "sink");

    if (!pipeline || !source || !convert || !sink) {
        std::cerr << "Failed to create GStreamer elements." << std::endl;
        if (pipeline) gst_object_unref(GST_OBJECT(pipeline));
        if (source) gst_object_unref(GST_OBJECT(source));
        if (convert) gst_object_unref(GST_OBJECT(convert));
        if (sink) gst_object_unref(GST_OBJECT(sink));
        return cv::Mat();
    }

    // Set up source caps for BGR input
    GstCaps* srcCaps = gst_caps_new_simple("video/x-raw",
        "format", G_TYPE_STRING, "BGR",
        "width", G_TYPE_INT, inputImage.cols,
        "height", G_TYPE_INT, inputImage.rows,
        "framerate", GST_TYPE_FRACTION, 30, 1,
        nullptr);
    g_object_set(G_OBJECT(source), "caps", srcCaps, nullptr);
    g_object_set(G_OBJECT(source), "is-live", TRUE, "block", TRUE, nullptr);

    // Set up sink caps for GRAY8 output
    GstCaps* sinkCaps = gst_caps_new_simple("video/x-raw",
        "format", G_TYPE_STRING, "GRAY8",
        nullptr);
    g_object_set(G_OBJECT(sink), "caps", sinkCaps, nullptr);
    g_object_set(G_OBJECT(sink), "sync", FALSE, "emit-signals", TRUE, nullptr);

    gst_bin_add_many(GST_BIN(pipeline), source, convert, sink, nullptr);
    if (!gst_element_link_many(source, convert, sink, nullptr)) {
        std::cerr << "Failed to link GStreamer elements." << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    // Create a buffer for the input image
    GstBuffer* buffer = gst_buffer_new_allocate(nullptr, inputImage.total() * inputImage.elemSize(), nullptr);
    GstMapInfo map;
    gst_buffer_map(buffer, &map, GST_MAP_WRITE);
    std::memcpy(map.data, inputImage.data, inputImage.total() * inputImage.elemSize());
    gst_buffer_unmap(buffer, &map);

    GstFlowReturn ret;
    g_signal_emit_by_name(source, "push-buffer", buffer, &ret);
    gst_buffer_unref(buffer);

    if (ret != GST_FLOW_OK) {
        std::cerr << "Failed to push buffer to appsrc." << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    // Set pipeline to PLAYING state
    gst_element_set_state(pipeline, GST_STATE_PLAYING);

    // Wait for processing to complete
    GstStateChangeReturn stateChangeRet;
    do {
        stateChangeRet = gst_element_get_state(pipeline, nullptr, nullptr, GST_CLOCK_TIME_NONE);
        if (stateChangeRet == GST_STATE_CHANGE_FAILURE) {
            std::cerr << "GStreamer pipeline failed!" << std::endl;
            gst_element_set_state(pipeline, GST_STATE_NULL);
            gst_object_unref(GST_OBJECT(pipeline));
            return cv::Mat();
        }
    } while (stateChangeRet != GST_STATE_CHANGE_SUCCESS);

    // Pull sample from appsink
    sample = gst_app_sink_pull_sample(GST_APP_SINK(sink));
    if (!sample) {
        std::cerr << "Failed to pull sample from appsink." << std::endl;
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    GstBuffer* outputBuffer = gst_sample_get_buffer(sample);
    if (!outputBuffer) {
        std::cerr << "Failed to get buffer from sample." << std::endl;
        gst_sample_unref(sample);
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    GstCaps* caps = gst_sample_get_caps(sample);
    if (!caps) {
        std::cerr << "Failed to get caps from sample." << std::endl;
        gst_sample_unref(sample);
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    GstMapInfo outputMap;
    gst_buffer_map(outputBuffer, &outputMap, GST_MAP_READ);

    // Extract width and height from caps
    GstStructure* s = gst_caps_get_structure(caps, 0);
    gint width, height;
    if (!gst_structure_get_int(s, "width", &width) || !gst_structure_get_int(s, "height", &height)) {
        std::cerr << "Failed to extract width and height from caps." << std::endl;
        gst_buffer_unmap(outputBuffer, &outputMap);
        gst_sample_unref(sample);
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    // Ensure cv::Mat is created correctly
    cv::Mat outputImage(height, width, CV_8UC1, outputMap.data);

    // Unmap and clean up
    gst_buffer_unmap(outputBuffer, &outputMap);
    gst_sample_unref(sample);
    gst_element_set_state(pipeline, GST_STATE_NULL);
    gst_object_unref(GST_OBJECT(pipeline));

    std::cout << "OutputImage Size: [" << outputImage.cols << " x " << outputImage.rows << "]" << std::endl;

    return outputImage;
}

cv::Mat ImageProcessorGStreamer::rotate(cv::Mat& inputImage, bool isRight) {
    if (inputImage.empty()) {
        std::cerr << "Input image is empty." << std::endl;
        return cv::Mat(); // Return empty image
    }

    GstElement* pipeline = nullptr;
    GstElement* source = nullptr;
    GstElement* convert = nullptr;
    GstElement* flip = nullptr;
    GstElement* sink = nullptr;
    GstSample* sample = nullptr;

    gst_init(nullptr, nullptr);

    // Create GStreamer elements
    pipeline = gst_pipeline_new("pipeline");
    source = gst_element_factory_make("appsrc", "source");
    convert = gst_element_factory_make("videoconvert", "convert");
    flip = gst_element_factory_make("videoflip", "flip");
    sink = gst_element_factory_make("appsink", "sink");

    if (!pipeline || !source || !convert || !flip || !sink) {
        std::cerr << "Failed to create GStreamer elements." << std::endl;
        if (pipeline) gst_object_unref(GST_OBJECT(pipeline));
        if (source) gst_object_unref(GST_OBJECT(source));
        if (convert) gst_object_unref(GST_OBJECT(convert));
        if (flip) gst_object_unref(GST_OBJECT(flip));
        if (sink) gst_object_unref(GST_OBJECT(sink));
        return cv::Mat();
    }

    // Set flip method for rotation
    g_object_set(G_OBJECT(flip), "method", isRight ? 1 : 3, nullptr); // 1 for 90 degrees clockwise, 3 for 90 degrees counter-clockwise

    // Set appsrc properties
    GstCaps* srcCaps = gst_caps_new_simple("video/x-raw",
        "format", G_TYPE_STRING, "BGR",
        "width", G_TYPE_INT, inputImage.cols,
        "height", G_TYPE_INT, inputImage.rows,
        "framerate", GST_TYPE_FRACTION, 30, 1,
        nullptr);
    g_object_set(G_OBJECT(source), "caps", srcCaps, nullptr);
    g_object_set(G_OBJECT(source), "is-live", TRUE, nullptr);

    // Set appsink properties
    GstCaps* sinkCaps = gst_caps_new_simple("video/x-raw",
        "format", G_TYPE_STRING, "BGR",
        nullptr);
    g_object_set(G_OBJECT(sink), "caps", sinkCaps, nullptr);
    g_object_set(G_OBJECT(sink), "sync", FALSE, "emit-signals", TRUE, nullptr);

    gst_bin_add_many(GST_BIN(pipeline), source, convert, flip, sink, nullptr);
    if (!gst_element_link_many(source, convert, flip, sink, nullptr)) {
        std::cerr << "Failed to link GStreamer elements." << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    // Convert cv::Mat to GstBuffer and push to appsrc
    GstBuffer* buffer = matToGstBuffer(inputImage);
    GstFlowReturn ret;
    g_signal_emit_by_name(source, "push-buffer", buffer, &ret);
    gst_buffer_unref(buffer);

    if (ret != GST_FLOW_OK) {
        std::cerr << "Failed to push buffer to appsrc." << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    gst_element_set_state(pipeline, GST_STATE_PLAYING);

    // Wait for the pipeline to be in PAUSED state
    GstStateChangeReturn stateChangeRet;
    do {
        stateChangeRet = gst_element_get_state(pipeline, nullptr, nullptr, GST_CLOCK_TIME_NONE);
        if (stateChangeRet == GST_STATE_CHANGE_FAILURE) {
            std::cerr << "GStreamer pipeline failed!" << std::endl;
            gst_element_set_state(pipeline, GST_STATE_NULL);
            gst_object_unref(GST_OBJECT(pipeline));
            return cv::Mat();
        }
    } while (stateChangeRet != GST_STATE_CHANGE_SUCCESS);

    // Wait for processing to complete
    gst_element_set_state(pipeline, GST_STATE_PAUSED);
    GstState state;
    GstState pending;
    do {
        gst_element_get_state(pipeline, &state, &pending, GST_CLOCK_TIME_NONE);
    } while (state != GST_STATE_PAUSED && pending != GST_STATE_VOID_PENDING);

    // Pull sample from appsink
    sample = gst_app_sink_pull_sample(GST_APP_SINK(sink));
    if (!sample) {
        std::cerr << "Failed to pull sample from appsink." << std::endl;
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    GstBuffer* outputBuffer = gst_sample_get_buffer(sample);
    if (!outputBuffer) {
        std::cerr << "Failed to get buffer from sample." << std::endl;
        gst_sample_unref(sample);
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    GstCaps* caps = gst_sample_get_caps(sample);
    if (!caps) {
        std::cerr << "Failed to get caps from sample." << std::endl;
        gst_sample_unref(sample);
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    cv::Mat outputImage = gstBufferToMat(outputBuffer, caps);

    gst_sample_unref(sample);
    gst_element_set_state(pipeline, GST_STATE_NULL);
    gst_object_unref(GST_OBJECT(pipeline));

    return outputImage;
}

cv::Mat ImageProcessorGStreamer::zoom(cv::Mat& inputImage, double newWidth, double newHeight) {

    GstElement* pipeline = nullptr;
    GstElement* source = nullptr;
    GstElement* convert = nullptr;
    GstElement* sink = nullptr;
    GstSample* sample = nullptr;

    gst_init(nullptr, nullptr);

    pipeline = gst_pipeline_new("pipeline");
    source = gst_element_factory_make("appsrc", "source");
    convert = gst_element_factory_make("videoconvert", "convert");
    sink = gst_element_factory_make("appsink", "sink");

    if (!pipeline || !source || !convert || !sink) {
        std::cerr << "GStreamer 요소를 생성하지 못했습니다." << std::endl;
        if (pipeline) gst_object_unref(GST_OBJECT(pipeline));
        if (source) gst_object_unref(GST_OBJECT(source));
        if (convert) gst_object_unref(GST_OBJECT(convert));
        if (sink) gst_object_unref(GST_OBJECT(sink));
        return cv::Mat();
    }

    // appsrc 설정
    GstCaps* srcCaps = gst_caps_new_simple("video/x-raw",
        "format", G_TYPE_STRING, "BGR",
        "width", G_TYPE_INT, inputImage.cols,
        "height", G_TYPE_INT, inputImage.rows,
        "framerate", GST_TYPE_FRACTION, 30, 1,
        nullptr);
    g_object_set(G_OBJECT(source), "caps", srcCaps, nullptr);
    g_object_set(G_OBJECT(source), "is-live", TRUE, "block", TRUE, nullptr);

    // appsink 설정
    GstCaps* sinkCaps = gst_caps_new_simple("video/x-raw",
        "format", G_TYPE_STRING, "BGR",
        nullptr);
    g_object_set(G_OBJECT(sink), "caps", sinkCaps, nullptr);
    g_object_set(G_OBJECT(sink), "sync", FALSE, "emit-signals", TRUE, nullptr);

    gst_bin_add_many(GST_BIN(pipeline), source, convert, sink, nullptr);
    if (!gst_element_link_many(source, convert, sink, nullptr)) {
        std::cerr << "요소들을 연결할 수 없습니다." << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    // cv::Mat을 GstBuffer로 변환하고 appsrc에 푸시
    GstBuffer* buffer = matToGstBuffer(inputImage);
    GstFlowReturn ret;
    g_signal_emit_by_name(source, "push-buffer", buffer, &ret);
    gst_buffer_unref(buffer);

    if (ret != GST_FLOW_OK) {
        std::cerr << "appsrc에 버퍼를 푸시하는데 실패했습니다." << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    gst_element_set_state(pipeline, GST_STATE_PLAYING);

    // GStreamer 파이프라인이 처리 완료될 때까지 기다림
    GstStateChangeReturn stateChangeRet;
    do {
        stateChangeRet = gst_element_get_state(pipeline, nullptr, nullptr, GST_CLOCK_TIME_NONE);
        if (stateChangeRet == GST_STATE_CHANGE_FAILURE) {
            std::cerr << "GStreamer 파이프라인 상태 변경 실패." << std::endl;
            gst_element_set_state(pipeline, GST_STATE_NULL);
            gst_object_unref(GST_OBJECT(pipeline));
            return cv::Mat();
        }
    } while (stateChangeRet != GST_STATE_CHANGE_SUCCESS);

    // appsink에서 샘플을 가져옵니다.
    sample = gst_app_sink_pull_sample(GST_APP_SINK(sink));
    if (!sample) {
        std::cerr << "appsink에서 샘플을 가져오는 데 실패했습니다." << std::endl;
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    GstBuffer* outputBuffer = gst_sample_get_buffer(sample);
    if (!outputBuffer) {
        std::cerr << "샘플에서 버퍼를 가져오는 데 실패했습니다." << std::endl;
        gst_sample_unref(sample);
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    GstCaps* caps = gst_sample_get_caps(sample);
    if (!caps) {
        std::cerr << "샘플에서 캡을 가져오는 데 실패했습니다." << std::endl;
        gst_sample_unref(sample);
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    cv::Mat outputImage = gstBufferToMat(outputBuffer, caps);

    gst_sample_unref(sample);
    gst_element_set_state(pipeline, GST_STATE_NULL);
    gst_object_unref(GST_OBJECT(pipeline));

    // 이미지 줌 처리
    cv::Mat zoomedImage;
    cv::resize(outputImage, zoomedImage, cv::Size(newWidth, newHeight), 0, 0, cv::INTER_LINEAR);

    return zoomedImage;
}

cv::Mat ImageProcessorGStreamer::gaussianBlur(cv::Mat& inputImage, int kernelSize) {
    GstElement* pipeline = nullptr;
    GstElement* source = nullptr;
    GstElement* convert = nullptr;
    GstElement* sink = nullptr;
    GstSample* sample = nullptr;

    gst_init(nullptr, nullptr);

    pipeline = gst_pipeline_new("pipeline");
    source = gst_element_factory_make("appsrc", "source");
    convert = gst_element_factory_make("videoconvert", "convert");
    sink = gst_element_factory_make("appsink", "sink");

    if (!pipeline || !source || !convert || !sink) {
        std::cerr << "GStreamer failed" << std::endl;
        return cv::Mat();
    }

    std::cout << "GStreamer 파이프라인 요소 생성 성공." << std::endl;

    // 입력 이미지의 캡 설정
    GstCaps* srcCaps = nullptr;
    switch (inputImage.type()) {
    case CV_8UC1:
        srcCaps = gst_caps_new_simple("video/x-raw",
            "format", G_TYPE_STRING, "GRAY8",
            "width", G_TYPE_INT, inputImage.cols,
            "height", G_TYPE_INT, inputImage.rows,
            "framerate", GST_TYPE_FRACTION, 30, 1,
            nullptr);
        break;
    case CV_8UC3:
        srcCaps = gst_caps_new_simple("video/x-raw",
            "format", G_TYPE_STRING, "BGR",
            "width", G_TYPE_INT, inputImage.cols,
            "height", G_TYPE_INT, inputImage.rows,
            "framerate", GST_TYPE_FRACTION, 30, 1,
            nullptr);
        break;
    default:
        std::cerr << "지원하지 않는 이미지 타입입니다: " << inputImage.type() << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    std::cout << "Source caps set: " << gst_caps_to_string(srcCaps) << std::endl;
    g_object_set(G_OBJECT(source), "caps", srcCaps, nullptr);
    g_object_set(G_OBJECT(source), "is-live", TRUE, nullptr);

    GstCaps* sinkCaps = gst_caps_new_simple("video/x-raw",
        "format", G_TYPE_STRING, inputImage.type() == CV_8UC1 ? "GRAY8" : "BGR",
        nullptr);
    std::cout << "Sink caps set: " << gst_caps_to_string(sinkCaps) << std::endl;
    g_object_set(G_OBJECT(sink), "caps", sinkCaps, nullptr);
    g_object_set(G_OBJECT(sink), "sync", TRUE, "emit-signals", TRUE, nullptr);

    gst_bin_add_many(GST_BIN(pipeline), source, convert, sink, nullptr);
    if (!gst_element_link_many(source, convert, sink, nullptr)) {
        std::cerr << "요소들을 연결할 수 없습니다." << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    // OpenCV를 사용하여 Gaussian Blur 적용
    cv::Mat blurredImage;
    cv::GaussianBlur(inputImage, blurredImage, cv::Size(kernelSize, kernelSize), 0);

    // 처리된 이미지를 GStreamer 파이프라인으로 푸시
    GstBuffer* buffer = matToGstBuffer(blurredImage);
    GstFlowReturn ret;
    g_signal_emit_by_name(source, "push-buffer", buffer, &ret);
    gst_buffer_unref(buffer);

    if (ret != GST_FLOW_OK) {
        std::cerr << "appsrc에 버퍼를 푸시하는데 실패했습니다. 반환값: " << ret << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    gst_element_set_state(pipeline, GST_STATE_PLAYING);
    std::cout << "Pipeline 상태를 PLAYING으로 설정했습니다." << std::endl;

    GstStateChangeReturn stateChangeRet;
    do {
        stateChangeRet = gst_element_get_state(pipeline, nullptr, nullptr, GST_CLOCK_TIME_NONE);
        std::cout << "Pipeline 상태 변경: " << stateChangeRet << std::endl;
    } while (stateChangeRet != GST_STATE_CHANGE_SUCCESS);

    sample = gst_app_sink_pull_sample(GST_APP_SINK(sink));
    if (!sample) {
        std::cerr << "appsink에서 샘플을 가져오는 데 실패했습니다." << std::endl;
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    GstBuffer* outputBuffer = gst_sample_get_buffer(sample);
    if (!outputBuffer) {
        std::cerr << "샘플에서 버퍼를 가져오는 데 실패했습니다." << std::endl;
        gst_sample_unref(sample);
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    GstCaps* caps = gst_sample_get_caps(sample);
    if (!caps) {
        std::cerr << "샘플에서 캡을 가져오는 데 실패했습니다." << std::endl;
        gst_sample_unref(sample);
        gst_element_set_state(pipeline, GST_STATE_NULL);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    cv::Mat outputImage = gstBufferToMat(outputBuffer, caps);

    printImagePixels(outputImage, 20);

    std::cout << "Output image type: " << outputImage.type() << std::endl;

    gst_sample_unref(sample);
    gst_element_set_state(pipeline, GST_STATE_NULL);
    gst_object_unref(GST_OBJECT(pipeline));

    printImagePixels(outputImage, 20);

    return outputImage;
}

cv::Mat ImageProcessorGStreamer::cannyEdges(cv::Mat& inputImage) {

    cv::Mat grayImage;
    cv::Mat edges;
    cv::Mat outputImage;

    // 입력 이미지가 컬러일 경우 그레이스케일로 변환
    if (inputImage.channels() == 3) {
        cv::cvtColor(inputImage, grayImage, cv::COLOR_BGR2GRAY);
    }
    else {
        grayImage = inputImage.clone();
    }

    // Canny 엣지 검출 수행
    cv::Canny(grayImage, edges, 50, 150);

    // Canny 엣지 결과를 출력 이미지로 변환
    if (inputImage.channels() == 3) {
        // 컬러 이미지에 초록색 엣지 표시
        outputImage = inputImage.clone();
        drawEdgesOnColorImage(outputImage, edges);
    }
    else {
        // 그레이스케일 이미지에 흰색 엣지 표시
        outputImage.create(edges.size(), CV_8UC1);
        drawEdgesOnGrayImage(outputImage, edges);
    }

    return outputImage;
}

cv::Mat ImageProcessorGStreamer::medianFilter(cv::Mat& inputImage) {

    cv::Mat outputImage;

    if (inputImage.empty()) {
        std::cerr << "Input image is empty." << std::endl;
        return outputImage; // 빈 이미지 반환
    }

    int numChannels = inputImage.channels();
    int kernelSize = 5; // 미디안 필터 커널 크기

    // 그레이스케일 이미지 처리
    if (numChannels == 1) {
        cv::medianBlur(inputImage, outputImage, kernelSize);
    }
    // 컬러 이미지 처리
    else if (numChannels == 3) {
        std::vector<cv::Mat> channels(3);
        cv::split(inputImage, channels);

        for (int i = 0; i < 3; ++i) {
            cv::medianBlur(channels[i], channels[i], kernelSize);
        }

        cv::merge(channels, outputImage);
    }
    else {
        std::cerr << "Unsupported number of channels: " << numChannels << std::endl;
    }

    return outputImage;
}

cv::Mat ImageProcessorGStreamer::sobelFilter(cv::Mat& inputImage)
{
    GstElement* pipeline = nullptr;
    GstElement* source = nullptr;
    GstElement* convert = nullptr;
    GstElement* sink = nullptr;
    GstSample* sample = nullptr;

    if (inputImage.empty()) {
        std::cerr << "Input image is empty." << std::endl;
        return cv::Mat(); // 빈 이미지 반환
    }

    // Initialize GStreamer
    gst_init(nullptr, nullptr);

    // Create elements
    pipeline = gst_pipeline_new("pipeline");
    source = gst_element_factory_make("appsrc", "source");
    convert = gst_element_factory_make("videoconvert", "convert");
    sink = gst_element_factory_make("appsink", "sink");

    if (!pipeline || !source || !convert || !sink) {
        std::cerr << "Failed to create elements" << std::endl;
        if (pipeline) gst_object_unref(GST_OBJECT(pipeline));
        if (source) gst_object_unref(GST_OBJECT(source));
        if (convert) gst_object_unref(GST_OBJECT(convert));
        if (sink) gst_object_unref(GST_OBJECT(sink));
        return cv::Mat(); // Return empty Mat on failure
    }

    // Set appsrc properties
    g_object_set(G_OBJECT(source), "caps",
        gst_caps_new_simple("video/x-raw",
            "format", G_TYPE_STRING, "BGR",
            "width", G_TYPE_INT, inputImage.cols,
            "height", G_TYPE_INT, inputImage.rows,
            "framerate", GST_TYPE_FRACTION, 30, 1,
            nullptr), nullptr);
    g_object_set(G_OBJECT(source), "is-live", TRUE, "block", TRUE, nullptr);

    // Set appsink properties
    g_object_set(G_OBJECT(sink), "caps",
        gst_caps_new_simple("video/x-raw",
            "format", G_TYPE_STRING, "BGR",
            nullptr), nullptr);
    g_object_set(G_OBJECT(sink), "sync", FALSE, nullptr);

    // Build the pipeline
    gst_bin_add_many(GST_BIN(pipeline), source, convert, sink, nullptr);
    if (!gst_element_link_many(source, convert, sink, nullptr)) {
        std::cerr << "Elements could not be linked." << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    // Apply Sobel filter using OpenCV
    cv::Mat grayImage, gradX, gradY, absGradX, absGradY, outputImage;
    if (inputImage.channels() == 3) {
        // Convert color image to grayscale
        cv::cvtColor(inputImage, grayImage, cv::COLOR_BGR2GRAY);
    }
    else {
        grayImage = inputImage;
    }

    // Apply Sobel filter
    cv::Sobel(grayImage, gradX, CV_16S, 1, 0, 3);
    cv::convertScaleAbs(gradX, absGradX);
    cv::Sobel(grayImage, gradY, CV_16S, 0, 1, 3);
    cv::convertScaleAbs(gradY, absGradY);
    cv::addWeighted(absGradX, 0.5, absGradY, 0.5, 0, outputImage);

    // Convert cv::Mat to GstBuffer
    GstBuffer* buffer = gst_buffer_new_allocate(nullptr, outputImage.total() * outputImage.elemSize(), nullptr);
    GstMapInfo map;
    gst_buffer_map(buffer, &map, GST_MAP_WRITE);
    memcpy(map.data, outputImage.data, outputImage.total() * outputImage.elemSize());
    gst_buffer_unmap(buffer, &map);

    // Push buffer to appsrc
    GstFlowReturn ret;
    g_signal_emit_by_name(source, "push-buffer", buffer, &ret);
    gst_buffer_unref(buffer);

    if (ret != GST_FLOW_OK) {
        std::cerr << "Failed to push buffer to appsrc" << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat(); // Return empty Mat on failure
    }

    // Set the pipeline to playing
    gst_element_set_state(pipeline, GST_STATE_PLAYING);

    // Pull sample from appsink
    sample = gst_app_sink_pull_sample(GST_APP_SINK(sink));
    if (!sample) {
        std::cerr << "Failed to pull sample from appsink" << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat(); // Return empty Mat on failure
    }

    GstBuffer* outputBuffer = gst_sample_get_buffer(sample);
    if (!outputBuffer) {
        std::cerr << "Failed to get buffer from sample" << std::endl;
        gst_sample_unref(sample);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat(); // Return empty Mat on failure
    }

    gst_buffer_map(outputBuffer, &map, GST_MAP_READ);
    cv::Mat finalImage(inputImage.rows, inputImage.cols, CV_8UC3, map.data);
    finalImage = finalImage.clone(); // Make a deep copy of the data
    gst_buffer_unmap(outputBuffer, &map);

    // Cleanup
    gst_sample_unref(sample);
    gst_element_set_state(pipeline, GST_STATE_NULL);
    gst_object_unref(pipeline);

    return finalImage;
}

cv::Mat ImageProcessorGStreamer::bilateralFilter(cv::Mat& inputImage)
{
    if (inputImage.empty()) {
        std::cerr << "입력 이미지가 비어 있습니다." << std::endl;
        return cv::Mat(); // 빈 Mat 반환
    }

    // GStreamer 요소 초기화
    GstElement* pipeline = nullptr;
    GstElement* source = nullptr;
    GstElement* convert = nullptr;
    GstElement* sink = nullptr;
    GstSample* sample = nullptr;

    // GStreamer 초기화
    gst_init(nullptr, nullptr);

    // 요소 생성
    pipeline = gst_pipeline_new("pipeline");
    source = gst_element_factory_make("appsrc", "source");
    convert = gst_element_factory_make("videoconvert", "convert");
    sink = gst_element_factory_make("appsink", "sink");

    if (!pipeline || !source || !convert || !sink) {
        std::cerr << "failed" << std::endl;
        if (pipeline) gst_object_unref(GST_OBJECT(pipeline));
        if (source) gst_object_unref(GST_OBJECT(source));
        if (convert) gst_object_unref(GST_OBJECT(convert));
        if (sink) gst_object_unref(GST_OBJECT(sink));
        return cv::Mat(); // 실패 시 빈 Mat 반환
    }

    // appsrc 속성 설정
    g_object_set(G_OBJECT(source), "caps",
        gst_caps_new_simple("video/x-raw",
            "format", G_TYPE_STRING, (inputImage.channels() == 1) ? "GRAY8" : "BGR",
            "width", G_TYPE_INT, inputImage.cols,
            "height", G_TYPE_INT, inputImage.rows,
            "framerate", GST_TYPE_FRACTION, 30, 1,
            nullptr), nullptr);
    g_object_set(G_OBJECT(source), "is-live", TRUE, "block", TRUE, nullptr);

    // appsink 속성 설정
    g_object_set(G_OBJECT(sink), "caps",
        gst_caps_new_simple("video/x-raw",
            "format", G_TYPE_STRING, (inputImage.channels() == 1) ? "GRAY8" : "BGR",
            nullptr), nullptr);
    g_object_set(G_OBJECT(sink), "sync", FALSE, nullptr);

    // 파이프라인 구축
    gst_bin_add_many(GST_BIN(pipeline), source, convert, sink, nullptr);
    if (!gst_element_link_many(source, convert, sink, nullptr)) {
        std::cerr << "요소를 연결할 수 없습니다." << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat();
    }

    // 양방향 필터 적용
    cv::Mat filteredImage;
    cv::bilateralFilter(inputImage, filteredImage, 9, 75, 75, cv::BORDER_DEFAULT);

    // cv::Mat을 GstBuffer로 변환
    GstBuffer* buffer = gst_buffer_new_allocate(nullptr, filteredImage.total() * filteredImage.elemSize(), nullptr);
    GstMapInfo map;
    gst_buffer_map(buffer, &map, GST_MAP_WRITE);
    memcpy(map.data, filteredImage.data, filteredImage.total() * filteredImage.elemSize());
    gst_buffer_unmap(buffer, &map);

    // 버퍼를 appsrc로 푸시
    GstFlowReturn ret;
    g_signal_emit_by_name(source, "push-buffer", buffer, &ret);
    gst_buffer_unref(buffer);

    if (ret != GST_FLOW_OK) {
        std::cerr << "appsrc로 버퍼를 푸시하는 데 실패했습니다." << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat(); // 실패 시 빈 Mat 반환
    }

    // 파이프라인을 재생 상태로 설정
    gst_element_set_state(pipeline, GST_STATE_PLAYING);

    // appsink에서 샘플 가져오기
    sample = gst_app_sink_pull_sample(GST_APP_SINK(sink));
    if (!sample) {
        std::cerr << "appsink에서 샘플을 가져오는 데 실패했습니다." << std::endl;
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat(); // 실패 시 빈 Mat 반환
    }

    GstBuffer* outputBuffer = gst_sample_get_buffer(sample);
    if (!outputBuffer) {
        std::cerr << "샘플에서 버퍼를 가져오는 데 실패했습니다." << std::endl;
        gst_sample_unref(sample);
        gst_object_unref(GST_OBJECT(pipeline));
        return cv::Mat(); // 실패 시 빈 Mat 반환
    }

    gst_buffer_map(outputBuffer, &map, GST_MAP_READ);
    cv::Mat outputImage(inputImage.rows, inputImage.cols, inputImage.type(), map.data);
    outputImage = outputImage.clone(); // 데이터의 깊은 복사본 만들기
    gst_buffer_unmap(outputBuffer, &map);

    // 정리
    gst_sample_unref(sample);
    gst_element_set_state(pipeline, GST_STATE_NULL);
    gst_object_unref(pipeline);

    return outputImage;
}

3. CMakeLists.txt
cmake_minimum_required(VERSION 3.14)
project(Project1 LANGUAGES CXX CUDA)

# Qt, OpenCV, CUDA 설정
set(CMAKE_PREFIX_PATH "C:/Qt/6.7.1/msvc2019_64" "C:/opencv/build")
find_package(Qt6 REQUIRED COMPONENTS Widgets Core Gui)
find_package(OpenCV REQUIRED COMPONENTS core imgproc highgui cudaarithm cudafilters cudawarping cudacodec cudafeatures2d cudaimgproc)
find_package(CUDA REQUIRED)

# IPP 설정
set(IPP_ROOT "C:/Program Files (x86)/Intel/oneAPI/ipp/2021.11")
include_directories("${IPP_ROOT}/include")
link_directories("${IPP_ROOT}/lib")

# GStreamer 설정
set(GSTREAMER_ROOT "C:/gstreamer/1.0/msvc_x86_64")
include_directories(
    "${GSTREAMER_ROOT}/include/gstreamer-1.0"
    "${GSTREAMER_ROOT}/include/glib-2.0"
    "${GSTREAMER_ROOT}/lib/glib-2.0/include"
)
link_directories("${GSTREAMER_ROOT}/lib")

# 추가 포함 디렉터리 설정
include_directories(
    ${CUDA_INCLUDE_DIRS}
    ${OpenCV_INCLUDE_DIRS}
    "${CUDA_TOOLKIT_ROOT_DIR}/include"
    ${IPP_ROOT}/include
    "${GSTREAMER_ROOT}/include/gstreamer-1.0"
    "${GSTREAMER_ROOT}/lib/gstreamer-1.0/include"
    "C:/myLab/Project1/Project1/imageProcessingLib"  # 헤더 파일 포함
)

# CUDA 파일 설정
set(CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS} -std=c++14 --expt-relaxed-constexpr -gencode arch=compute_86,code=sm_86)

# Qt 래핑 파일 생성
qt6_wrap_cpp(MOC_FILES
    MainWindow.h
    ImageProcessor.h
)

# CUDA 파일 컴파일 및 라이브러리 생성
cuda_add_library(image_processing
    imageProcessing.cu
)

# 라이브러리 디렉토리 설정
link_directories("${CMAKE_BINARY_DIR}")  # 라이브러리 디렉토리 설정

# 라이브러리 링크
target_link_libraries(image_processing
    ippcc.lib
    ippcore.lib
    ippvm.lib
    ipps.lib
    ippi.lib
    ippcv.lib
    nppial
    nppicc
    nppidei
    nppif
    nppig
    nppim
    nppist
    nppisu
    nppitc
    npps
    imageProcessingLib
)

# GStreamer 라이브러리 추가
list(APPEND GSTREAMER_LIBRARIES
    gstreamer-1.0.lib
    gobject-2.0.lib
    glib-2.0.lib
    gstapp-1.0.lib
    gstbase-1.0.lib
    gstvideo-1.0.lib
)

# 실행 파일 추가
add_executable(Project1
    main.cpp
    MainWindow.cpp
    MainWindow.h
    MainWindow.ui
    ImageProcessor.cpp
    ImageProcessor.h
    ${MOC_FILES}
)

# 라이브러리 링크
target_link_libraries(Project1
    Qt6::Widgets
    Qt6::Core
    Qt6::Gui
    ${OpenCV_LIBS}
    image_processing
    imageProcessingLib
    ${CUDA_LIBRARIES}
    ${CUDNN_LIBRARIES}
    ${GSTREAMER_LIBRARIES}
)

# 빌드 후 TARGET_FILE_DIR 출력
add_custom_command(TARGET Project1 POST_BUILD
    COMMAND ${CMAKE_COMMAND} -E echo "TARGET_FILE_DIR for Project1: $<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E echo "$<TARGET_FILE_DIR:Project1>" > "Project1_dir.txt"
)

# 실행 파일 출력 디렉토리 설정
set_target_properties(Project1 PROPERTIES
    RUNTIME_OUTPUT_DIRECTORY_DEBUG "${CMAKE_BINARY_DIR}/Debug"
    RUNTIME_OUTPUT_DIRECTORY_RELEASE "${CMAKE_BINARY_DIR}/Release"
)

#file(COPY "C:/myLab/Project1/Project1/imageProcessingLib/build/bin/Debug/imageProcessingLib.dll"
#     DESTINATION "${CMAKE_BINARY_DIR}/Debug"
#)

file(COPY "C:/myLab/Project1/Project1/imageProcessingLib/build/lib/Debug/imageProcessingLib.lib"
     DESTINATION "${CMAKE_BINARY_DIR}/Debug"
)

add_custom_command(TARGET Project1 POST_BUILD 
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/myLab/Project1/Project1/imageProcessingLib/build/bin/Debug/imageProcessingLib.dll"
    "$<TARGET_FILE_DIR:Project1>"
    COMMENT "Copying Debug imageProcessingLib dll to output directory"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/myLab/Project1/Project1/imageProcessingLib/build/lib/Debug/imageProcessingLib.lib"
    "$<TARGET_FILE_DIR:Project1>"
    COMMENT "Copying Debug imageProcessingLib lib to output directory"
)

# 디버그 빌드에서의 OpenCV opencv_world DLL 복사
add_custom_command(TARGET Project1 POST_BUILD
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/x64/vc16/bin/opencv_world4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMENT "Copying OpenCV Debug opencv_world DLL to output directory"
)

# 릴리스 빌드에서의 OpenCV opencv_world DLL 복사
add_custom_command(TARGET Project1 POST_BUILD
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/x64/vc16/bin/opencv_world4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMENT "Copying OpenCV Release opencv_world DLL to output directory"
)

# 디버그 빌드에서의 OpenCV DLL 복사
add_custom_command(TARGET Project1 POST_BUILD
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_cudaarithm4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_cudaimgproc4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_cudafilters4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_imgcodecs4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_core4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Debug/opencv_imgproc4100d.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMENT "Copying OpenCV Debug DLLs to output directory"
)

# 릴리스 빌드에서의 OpenCV DLL 복사
add_custom_command(TARGET Project1 POST_BUILD
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_cudaarithm4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_cudaimgproc4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_cudafilters4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_imgcodecs4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_core4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMAND ${CMAKE_COMMAND} -E copy_if_different
    "C:/opencv/build/bin/Release/opencv_imgproc4100.dll" "$<TARGET_FILE_DIR:Project1>"
    COMMENT "Copying OpenCV Release DLLs to output directory"
)

# 파일 인코딩 설정 추가
add_compile_options("$<$<CXX_COMPILER_ID:MSVC>:/utf-8>")

# OpenMP 설정
find_package(OpenMP REQUIRED)
if(OpenMP_CXX_FOUND)
    target_link_libraries(Project1 OpenMP::OpenMP_CXX)
endif()

if (MSVC)
    set_property(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY VS_STARTUP_PROJECT Project1)

    # Debug 빌드
    set(CMAKE_CXX_FLAGS_DEBUG "${CMAKE_CXX_FLAGS_DEBUG} /MDd")
    set(CMAKE_C_FLAGS_DEBUG "${CMAKE_C_FLAGS_DEBUG} /MDd")

    # Release 빌드
    set(CMAKE_CXX_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE} /MD")
    set(CMAKE_C_FLAGS_RELEASE "${CMAKE_C_FLAGS_RELEASE} /MD")

    # 반복자 디버그 레벨 설정
    add_compile_definitions(
        $<$<CONFIG:Debug>:_ITERATOR_DEBUG_LEVEL=2>
        $<$<CONFIG:Release>:_ITERATOR_DEBUG_LEVEL=0>
    )
endif()
